{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "#plt.style.use('fivethirtyeight')\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score,f1_score,roc_curve, auc,roc_auc_score,precision_score,recall_score,matthews_corrcoef\n",
    "\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import mutual_info_classif,f_classif\n",
    "np.set_printoptions(precision=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_ranking_selection(X_train, y_train, n_features):\n",
    "    '''\n",
    "    n_features: number of feature to select for training\n",
    "    \n",
    "    returns: feature name with pearson correlation coefficient(in descending) and selected n_features\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    df = X_train.copy()\n",
    "    df['label'] = y_train.values    \n",
    "    correlation_mat = df.corr(method = 'pearson')    \n",
    "    feature_name = list(correlation_mat.index)   \n",
    "    \n",
    "    ndf = pd.DataFrame()\n",
    "    ndf['feature'] = feature_name\n",
    "    ndf ['importance'] = abs((correlation_mat.iloc[:,-1]).values )\n",
    "    \n",
    "    mdf = ndf[:-1]    \n",
    "    mdf = (mdf.sort_values(by='importance', ascending=False)).reset_index(drop = True)\n",
    "    \n",
    "    if n_features > len(mdf):\n",
    "        print('Number features to select is too large.')\n",
    "        return mdf\n",
    "    else:        \n",
    "        selected_feature = list((mdf.iloc[0:n_features])['feature'].values)        \n",
    "        return mdf, selected_feature        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. CLR  202"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fature Ranking information\n",
      "---------------------------------------------------------------------------------------------\n",
      "              feature  importance\n",
      "0            t_std_Gx    0.373362\n",
      "1   t_Crest_factor_Gy    0.313880\n",
      "2   t_Crest_factor_Gz    0.275701\n",
      "3           t_mean_Az    0.225354\n",
      "4       t_kurtosis_Ay    0.198155\n",
      "5       t_kurtosis_Ax    0.171519\n",
      "6       t_kurtosis_Az    0.159185\n",
      "7           t_mean_Ay    0.156218\n",
      "8             f_Q1_Ay    0.134174\n",
      "9       t_skewness_Ay    0.105127\n",
      "10      t_kurtosis_Gy    0.101014\n",
      "11          t_mean_Ax    0.063295\n",
      "12      t_skewness_Gx    0.059554\n",
      "13  t_Crest_factor_Az    0.056868\n",
      "14          t_mean_Gy    0.051155\n",
      "15            f_Q1_Ax    0.048915\n",
      "16          t_mean_Gx    0.032505\n",
      "17      t_skewness_Az    0.029633\n",
      "18      t_skewness_Ax    0.023745\n",
      "19      t_kurtosis_Gz    0.023675\n",
      "20      t_skewness_Gy    0.020575\n",
      "21      f_variance_Gx    0.014517\n",
      "22      f_variance_Gy    0.012160\n",
      "23      f_variance_Gz    0.009442\n",
      "24          t_mean_Gz    0.008278\n",
      "25      t_skewness_Gz    0.004186\n",
      "26      t_kurtosis_Gx    0.000892\n",
      "['t_std_Gx', 't_Crest_factor_Gy', 't_Crest_factor_Gz', 't_mean_Az', 't_kurtosis_Ay', 't_kurtosis_Ax', 't_kurtosis_Az', 't_mean_Ay', 'f_Q1_Ay', 't_skewness_Ay', 't_kurtosis_Gy', 't_mean_Ax', 't_skewness_Gx', 't_Crest_factor_Az', 't_mean_Gy', 'f_Q1_Ax', 't_mean_Gx', 't_skewness_Az', 't_skewness_Ax', 't_kurtosis_Gz', 't_skewness_Gy', 'f_variance_Gx', 'f_variance_Gy', 'f_variance_Gz', 't_mean_Gz', 't_skewness_Gz', 't_kurtosis_Gx']\n",
      "---------------------------------------------------------------------------------------------\n",
      "\t\t\t\tClassifier: DECISION TREE\n",
      "---------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------\n",
      "\t\tBefore Feature Selection\tAfter Feature Selection\n",
      "No. of features:\t 202 \t\t\t 15\n",
      "Train Accuracy:\t\t 100.0 \t\t\t 99.29\n",
      "Test Accuracy:\t\t 99.87 \t\t\t 99.12\n",
      "ROC AUC score:\t\t 1.0 \t\t\t 0.99\n",
      "f1_score:\t\t 1.0 \t\t\t 0.99\n",
      "Precision:\t\t 1.0 \t\t\t 0.99\n",
      "Recall:\t\t\t 1.0 \t\t\t 0.99\n",
      "MCC:\t\t\t 1.0 \t\t\t 0.98\n",
      "Train Time (in seconds): 32.14 \t\t\t 0.72\n",
      "Confusion Matrix(Before):\n",
      " [[ 4492    14]\n",
      " [   10 13758]]\n",
      "Confusion Matrix(after):\n",
      " [[ 4434    72]\n",
      " [   89 13679]]\n",
      "---------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Load dataset as pandas data frame\n",
    "filename = 'CLR_both_202.csv'\n",
    "dataset = pd.read_csv(filename)\n",
    "\n",
    "#Split data into input and output variable\n",
    "X = dataset.iloc[:,0:dataset.shape[1]-1]\n",
    "Y = dataset.iloc[:,-1]\n",
    "\n",
    "X_trainB, X_testB, y_trainB, y_testB = train_test_split(X, Y,test_size=0.3,random_state = 100)\n",
    "X_trainA, X_testA, y_trainA, y_testA = train_test_split(X, Y,test_size=0.3,random_state = 100)\n",
    "\n",
    "# Removing Constant features\n",
    "constant_filter = VarianceThreshold()\n",
    "constant_filter.fit(X_trainA)\n",
    "constant_columns = [col for col in X_trainA.columns\n",
    "                    if col not in X_trainA.columns[constant_filter.get_support()]]\n",
    "X_trainA.drop(labels=constant_columns,axis=1, inplace=True) \n",
    "X_testA.drop(labels=constant_columns,axis=1, inplace=True)\n",
    "\n",
    "# Removing Quasi-Constant features\n",
    "qconstant_filter = VarianceThreshold(0.01)\n",
    "qconstant_filter.fit(X_trainA)\n",
    "qconstant_columns = [col for col in X_trainA.columns\n",
    "                    if col not in X_trainA.columns[qconstant_filter.get_support()]]\n",
    "X_trainA.drop(labels=qconstant_columns,axis=1, inplace=True) \n",
    "X_testA.drop(labels=qconstant_columns,axis=1, inplace=True)\n",
    "\n",
    "# Removing Correlated Features\n",
    "correlated_features = set()  \n",
    "correlation_matrix = X_trainA.corr(method = 'pearson')\n",
    "for i in range(len(correlation_matrix.columns)):  \n",
    "    for j in range(i):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > 0.4:\n",
    "            colname = correlation_matrix.columns[i]\n",
    "            correlated_features.add(colname)\n",
    "X_trainA.drop(labels=correlated_features,axis=1, inplace=True) \n",
    "X_testA.drop(labels=correlated_features,axis=1, inplace=True)\n",
    "\n",
    "# feature ranking and selection    \n",
    "ranking_info, selected_features = feature_ranking_selection(X_trainA, y_trainA, 15)\n",
    "print('Fature Ranking information')\n",
    "print('---------------------------------------------------------------------------------------------')\n",
    "print(ranking_info)\n",
    "print(list(ranking_info['feature']))\n",
    "print('---------------------------------------------------------------------------------------------')\n",
    "X_trainA = X_trainA[selected_features]        \n",
    "X_testA = X_testA[selected_features] \n",
    "\n",
    "#names = [\"Nearest Neighbors\",\"Decision Tree\",\"Naive Bayes\"]\n",
    "names = [\"Decision Tree\"]\n",
    "\n",
    "classifiers = [\n",
    "    #KNeighborsClassifier(5, n_jobs= -1 ),\n",
    "    DecisionTreeClassifier(random_state = 100)\n",
    "    #GaussianNB(),\n",
    "   ]\n",
    "classifier2= [\n",
    "    #KNeighborsClassifier(5, n_jobs= -1 ),\n",
    "    DecisionTreeClassifier(max_depth = 5, min_samples_leaf = 50,random_state = 100)\n",
    "    #GaussianNB(),\n",
    "   ]\n",
    "clf_bef = list()\n",
    "clf_aft = list()\n",
    "\n",
    "for name, clf, dlf in zip(names,classifiers,classifier2):\n",
    "    # clf for before\n",
    "    # dlf for after\n",
    "    \n",
    "    # Before Feature Selection\n",
    "    startB = time.time()\n",
    "    clf.fit(X_trainB,y_trainB)    \n",
    "    endB = time.time() \n",
    "    clf_bef.append(clf)  \n",
    "    \n",
    "    # after Feature Selection    \n",
    "    startA = time.time()\n",
    "    dlf.fit(X_trainA,y_trainA)\n",
    "    endA = time.time()\n",
    "    clf_aft.append(dlf)\n",
    "        \n",
    "    print('\\t\\t\\t\\tClassifier:',name.upper())\n",
    "    print('---------------------------------------------------------------------------------------------')\n",
    "    print('---------------------------------------------------------------------------------------------')\n",
    "         \n",
    "    print('\\t\\tBefore Feature Selection\\tAfter Feature Selection')\n",
    "    print('No. of features:\\t', X_trainB.shape[1],'\\t\\t\\t',X_trainA.shape[1])\n",
    "    #print(\"Dataset Size(in MB):\\t\",(X_trainB.values.nbytes/1e6),'\\t\\t',(X_trainA.values.nbytes/1e6))\n",
    "\n",
    "    # training accuracy\n",
    "    train_predB = clf.predict(X_trainB)\n",
    "    train_predA = dlf.predict(X_trainA)\n",
    "    train_accB = round(accuracy_score(y_trainB,train_predB)*100, 2)\n",
    "    train_accA = round(accuracy_score(y_trainA,train_predA)*100, 2)\n",
    "    print('Train Accuracy:\\t\\t',train_accB,'\\t\\t\\t',train_accA)\n",
    "\n",
    "    # test accuracy \n",
    "    test_predB = clf.predict(X_testB)\n",
    "    test_predA = dlf.predict(X_testA)\n",
    "    test_accB = round(accuracy_score(y_testB,test_predB)*100, 2)\n",
    "    test_accA = round(accuracy_score(y_testA,test_predA)*100, 2)\n",
    "    print('Test Accuracy:\\t\\t',test_accB,'\\t\\t\\t',test_accA)\n",
    "        \n",
    "    # roc_auc_score\n",
    "    test_roc_aucB = round(roc_auc_score(y_testB,test_predB), 2)\n",
    "    test_roc_aucA = round(roc_auc_score(y_testA,test_predA), 2)\n",
    "    print('ROC AUC score:\\t\\t',test_roc_aucB,'\\t\\t\\t',test_roc_aucA)\n",
    "\n",
    "    # f1 score\n",
    "    test_f1B = round(f1_score(y_testB,test_predB),2)\n",
    "    test_f1A = round(f1_score(y_testA,test_predA),2)\n",
    "    print('f1_score:\\t\\t',test_f1B,'\\t\\t\\t',test_f1A)\n",
    "\n",
    "    # precision\n",
    "    test_precB = round(precision_score(y_testB,test_predB),2)\n",
    "    test_precA = round(precision_score(y_testA,test_predA),2)\n",
    "    print('Precision:\\t\\t',test_precB,'\\t\\t\\t',test_precA)\n",
    "\n",
    "    # recall\n",
    "    test_recallB = round(recall_score(y_testB,test_predB),2)\n",
    "    test_recallA = round(recall_score(y_testA,test_predA),2)\n",
    "    print('Recall:\\t\\t\\t',test_recallB,'\\t\\t\\t',test_recallA)\n",
    "\n",
    "    # Matthews correlation coefficient\n",
    "    test_MCCB = round(matthews_corrcoef(y_testB,test_predB),2)\n",
    "    test_MCCA = round(matthews_corrcoef(y_testA,test_predA),2)\n",
    "    print('MCC:\\t\\t\\t',test_MCCB,'\\t\\t\\t',test_MCCA)\n",
    "           \n",
    "    # training time\n",
    "    timeB = round((float(endB)- float(startB)),2)\n",
    "    timeA = round((float(endA)- float(startA)),2)\n",
    "    print('Train Time (in seconds):',timeB,'\\t\\t\\t',timeA)\n",
    "       \n",
    "    # confusion matrix\n",
    "    cm_resultB = confusion_matrix(y_testB,test_predB)\n",
    "    cm_resultA = confusion_matrix(y_testA,test_predA)\n",
    "    print('Confusion Matrix(Before):\\n',cm_resultB)\n",
    "    print('Confusion Matrix(after):\\n',cm_resultA)\n",
    "           \n",
    "    print('---------------------------------------------------------------------------------------------')\n",
    "    print('---------------------------------------------------------------------------------------------')\n",
    "    print('\\n')   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. C 202"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fature Ranking information\n",
      "---------------------------------------------------------------------------------------------\n",
      "              feature  importance\n",
      "0           t_mean_Ay    0.892635\n",
      "1   t_Crest_factor_Gz    0.366428\n",
      "2   t_Crest_factor_Gy    0.330982\n",
      "3           t_mean_Az    0.265255\n",
      "4       t_kurtosis_Az    0.199405\n",
      "5       t_skewness_Ay    0.196483\n",
      "6       t_skewness_Gx    0.134665\n",
      "7       t_kurtosis_Gz    0.111864\n",
      "8       f_variance_Gx    0.111046\n",
      "9       t_kurtosis_Ax    0.093200\n",
      "10      t_kurtosis_Gx    0.081011\n",
      "11          t_mean_Gy    0.078394\n",
      "12           f_IQR_Az    0.077315\n",
      "13      t_kurtosis_Gy    0.070696\n",
      "14      t_skewness_Gy    0.043170\n",
      "15      t_skewness_Az    0.032906\n",
      "16      t_skewness_Ax    0.013089\n",
      "17      t_skewness_Gz    0.010383\n",
      "18      f_variance_Gy    0.009470\n",
      "19  t_Crest_factor_Az    0.008228\n",
      "20          t_mean_Gx    0.004855\n",
      "21          t_mean_Gz    0.004785\n",
      "22      t_variance_Ax    0.003958\n",
      "['t_mean_Ay', 't_Crest_factor_Gz', 't_Crest_factor_Gy', 't_mean_Az', 't_kurtosis_Az', 't_skewness_Ay', 't_skewness_Gx', 't_kurtosis_Gz', 'f_variance_Gx', 't_kurtosis_Ax', 't_kurtosis_Gx', 't_mean_Gy', 'f_IQR_Az', 't_kurtosis_Gy', 't_skewness_Gy', 't_skewness_Az', 't_skewness_Ax', 't_skewness_Gz', 'f_variance_Gy', 't_Crest_factor_Az', 't_mean_Gx', 't_mean_Gz', 't_variance_Ax']\n",
      "---------------------------------------------------------------------------------------------\n",
      "\t\t\t\tClassifier: DECISION TREE\n",
      "---------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------\n",
      "\t\tBefore Feature Selection\tAfter Feature Selection\n",
      "No. of features:\t 202 \t\t\t 15\n",
      "Train Accuracy:\t\t 100.0 \t\t\t 99.35\n",
      "Test Accuracy:\t\t 99.67 \t\t\t 99.28\n",
      "ROC AUC score:\t\t 1.0 \t\t\t 0.99\n",
      "f1_score:\t\t 1.0 \t\t\t 1.0\n",
      "Precision:\t\t 1.0 \t\t\t 0.99\n",
      "Recall:\t\t\t 1.0 \t\t\t 1.0\n",
      "MCC:\t\t\t 0.99 \t\t\t 0.98\n",
      "Train Time (in seconds): 3.12 \t\t\t 0.17\n",
      "Confusion Matrix(Before):\n",
      " [[1554    8]\n",
      " [  12 4579]]\n",
      "Confusion Matrix(after):\n",
      " [[1532   30]\n",
      " [  14 4577]]\n",
      "---------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Load dataset as pandas data frame\n",
    "filename = \"centre_both.csv\"\n",
    "dataset = pd.read_csv(filename)\n",
    "\n",
    "#Split data into input and output variable\n",
    "X = dataset.iloc[:,0:dataset.shape[1]-1]\n",
    "Y = dataset.iloc[:,-1]\n",
    "\n",
    "X_trainB, X_testB, y_trainB, y_testB = train_test_split(X, Y,test_size=0.3,random_state = 100)\n",
    "X_trainA, X_testA, y_trainA, y_testA = train_test_split(X, Y,test_size=0.3,random_state = 100)\n",
    "\n",
    "# Removing Constant features\n",
    "constant_filter = VarianceThreshold()\n",
    "constant_filter.fit(X_trainA)\n",
    "constant_columns = [col for col in X_trainA.columns\n",
    "                    if col not in X_trainA.columns[constant_filter.get_support()]]\n",
    "X_trainA.drop(labels=constant_columns,axis=1, inplace=True) \n",
    "X_testA.drop(labels=constant_columns,axis=1, inplace=True)\n",
    "\n",
    "# Removing Quasi-Constant features\n",
    "qconstant_filter = VarianceThreshold(0.01)\n",
    "qconstant_filter.fit(X_trainA)\n",
    "qconstant_columns = [col for col in X_trainA.columns\n",
    "                    if col not in X_trainA.columns[qconstant_filter.get_support()]]\n",
    "X_trainA.drop(labels=qconstant_columns,axis=1, inplace=True) \n",
    "X_testA.drop(labels=qconstant_columns,axis=1, inplace=True)\n",
    "\n",
    "# Removing Correlated Features\n",
    "correlated_features = set()  \n",
    "correlation_matrix = X_trainA.corr(method = 'pearson')\n",
    "for i in range(len(correlation_matrix.columns)):  \n",
    "    for j in range(i):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > 0.4:\n",
    "            colname = correlation_matrix.columns[i]\n",
    "            correlated_features.add(colname)\n",
    "X_trainA.drop(labels=correlated_features,axis=1, inplace=True) \n",
    "X_testA.drop(labels=correlated_features,axis=1, inplace=True)\n",
    "\n",
    "# feature ranking and selection    \n",
    "ranking_info, selected_features = feature_ranking_selection(X_trainA, y_trainA, 15)\n",
    "print('Fature Ranking information')\n",
    "print('---------------------------------------------------------------------------------------------')\n",
    "print(ranking_info)\n",
    "print(list(ranking_info['feature']))\n",
    "print('---------------------------------------------------------------------------------------------')\n",
    "X_trainA = X_trainA[selected_features]        \n",
    "X_testA = X_testA[selected_features] \n",
    "\n",
    "#names = [\"Nearest Neighbors\",\"Decision Tree\",\"Naive Bayes\"]\n",
    "names = [\"Decision Tree\"]\n",
    "\n",
    "classifiers = [\n",
    "    #KNeighborsClassifier(5, n_jobs= -1 ),\n",
    "    DecisionTreeClassifier(random_state = 100)\n",
    "    #GaussianNB(),\n",
    "   ]\n",
    "classifier2= [\n",
    "    #KNeighborsClassifier(5, n_jobs= -1 ),\n",
    "    DecisionTreeClassifier(max_depth = 5, min_samples_leaf = 50,random_state = 100)\n",
    "    #GaussianNB(),\n",
    "   ]\n",
    "clf_bef = list()\n",
    "clf_aft = list()\n",
    "\n",
    "for name, clf, dlf in zip(names,classifiers,classifier2):\n",
    "    # clf for before\n",
    "    # dlf for after\n",
    "    \n",
    "    # Before Feature Selection\n",
    "    startB = time.time()\n",
    "    clf.fit(X_trainB,y_trainB)    \n",
    "    endB = time.time() \n",
    "    clf_bef.append(clf)  \n",
    "    \n",
    "    # after Feature Selection    \n",
    "    startA = time.time()\n",
    "    dlf.fit(X_trainA,y_trainA)\n",
    "    endA = time.time()\n",
    "    clf_aft.append(dlf)\n",
    "        \n",
    "    print('\\t\\t\\t\\tClassifier:',name.upper())\n",
    "    print('---------------------------------------------------------------------------------------------')\n",
    "    print('---------------------------------------------------------------------------------------------')\n",
    "         \n",
    "    print('\\t\\tBefore Feature Selection\\tAfter Feature Selection')\n",
    "    print('No. of features:\\t', X_trainB.shape[1],'\\t\\t\\t',X_trainA.shape[1])\n",
    "    #print(\"Dataset Size(in MB):\\t\",(X_trainB.values.nbytes/1e6),'\\t\\t',(X_trainA.values.nbytes/1e6))\n",
    "\n",
    "    # training accuracy\n",
    "    train_predB = clf.predict(X_trainB)\n",
    "    train_predA = dlf.predict(X_trainA)\n",
    "    train_accB = round(accuracy_score(y_trainB,train_predB)*100, 2)\n",
    "    train_accA = round(accuracy_score(y_trainA,train_predA)*100, 2)\n",
    "    print('Train Accuracy:\\t\\t',train_accB,'\\t\\t\\t',train_accA)\n",
    "\n",
    "    # test accuracy \n",
    "    test_predB = clf.predict(X_testB)\n",
    "    test_predA = dlf.predict(X_testA)\n",
    "    test_accB = round(accuracy_score(y_testB,test_predB)*100, 2)\n",
    "    test_accA = round(accuracy_score(y_testA,test_predA)*100, 2)\n",
    "    print('Test Accuracy:\\t\\t',test_accB,'\\t\\t\\t',test_accA)\n",
    "        \n",
    "    # roc_auc_score\n",
    "    test_roc_aucB = round(roc_auc_score(y_testB,test_predB), 2)\n",
    "    test_roc_aucA = round(roc_auc_score(y_testA,test_predA), 2)\n",
    "    print('ROC AUC score:\\t\\t',test_roc_aucB,'\\t\\t\\t',test_roc_aucA)\n",
    "\n",
    "    # f1 score\n",
    "    test_f1B = round(f1_score(y_testB,test_predB),2)\n",
    "    test_f1A = round(f1_score(y_testA,test_predA),2)\n",
    "    print('f1_score:\\t\\t',test_f1B,'\\t\\t\\t',test_f1A)\n",
    "\n",
    "    # precision\n",
    "    test_precB = round(precision_score(y_testB,test_predB),2)\n",
    "    test_precA = round(precision_score(y_testA,test_predA),2)\n",
    "    print('Precision:\\t\\t',test_precB,'\\t\\t\\t',test_precA)\n",
    "\n",
    "    # recall\n",
    "    test_recallB = round(recall_score(y_testB,test_predB),2)\n",
    "    test_recallA = round(recall_score(y_testA,test_predA),2)\n",
    "    print('Recall:\\t\\t\\t',test_recallB,'\\t\\t\\t',test_recallA)\n",
    "\n",
    "    # Matthews correlation coefficient\n",
    "    test_MCCB = round(matthews_corrcoef(y_testB,test_predB),2)\n",
    "    test_MCCA = round(matthews_corrcoef(y_testA,test_predA),2)\n",
    "    print('MCC:\\t\\t\\t',test_MCCB,'\\t\\t\\t',test_MCCA)\n",
    "           \n",
    "    # training time\n",
    "    timeB = round((float(endB)- float(startB)),2)\n",
    "    timeA = round((float(endA)- float(startA)),2)\n",
    "    print('Train Time (in seconds):',timeB,'\\t\\t\\t',timeA)\n",
    "       \n",
    "    # confusion matrix\n",
    "    cm_resultB = confusion_matrix(y_testB,test_predB)\n",
    "    cm_resultA = confusion_matrix(y_testA,test_predA)\n",
    "    print('Confusion Matrix(Before):\\n',cm_resultB)\n",
    "    print('Confusion Matrix(after):\\n',cm_resultA)\n",
    "           \n",
    "    print('---------------------------------------------------------------------------------------------')\n",
    "    print('---------------------------------------------------------------------------------------------')\n",
    "    print('\\n')   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. L 202"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fature Ranking information\n",
      "---------------------------------------------------------------------------------------------\n",
      "              feature  importance\n",
      "0            t_std_Gx    0.374021\n",
      "1   t_Crest_factor_Gy    0.300990\n",
      "2   t_Crest_factor_Gz    0.209767\n",
      "3       t_kurtosis_Ax    0.201654\n",
      "4           t_mean_Az    0.191012\n",
      "5       t_kurtosis_Ay    0.151359\n",
      "6       t_kurtosis_Az    0.139311\n",
      "7           t_mean_Ax    0.119069\n",
      "8       t_skewness_Gz    0.114926\n",
      "9       t_kurtosis_Gy    0.108051\n",
      "10          t_mean_Gz    0.099613\n",
      "11      t_skewness_Az    0.083611\n",
      "12      t_kurtosis_Gx    0.044731\n",
      "13            f_Q1_Ax    0.037596\n",
      "14      t_kurtosis_Gz    0.036529\n",
      "15            f_Q1_Ay    0.032553\n",
      "16          t_mean_Gy    0.030143\n",
      "17          t_mean_Gx    0.029421\n",
      "18          t_mean_Ay    0.024629\n",
      "19      f_variance_Gx    0.015039\n",
      "20      t_skewness_Gy    0.011618\n",
      "21      t_skewness_Gx    0.007027\n",
      "22      f_variance_Gy    0.003401\n",
      "23      t_skewness_Ax    0.002070\n",
      "['t_std_Gx', 't_Crest_factor_Gy', 't_Crest_factor_Gz', 't_kurtosis_Ax', 't_mean_Az', 't_kurtosis_Ay', 't_kurtosis_Az', 't_mean_Ax', 't_skewness_Gz', 't_kurtosis_Gy', 't_mean_Gz', 't_skewness_Az', 't_kurtosis_Gx', 'f_Q1_Ax', 't_kurtosis_Gz', 'f_Q1_Ay', 't_mean_Gy', 't_mean_Gx', 't_mean_Ay', 'f_variance_Gx', 't_skewness_Gy', 't_skewness_Gx', 'f_variance_Gy', 't_skewness_Ax']\n",
      "---------------------------------------------------------------------------------------------\n",
      "\t\t\t\tClassifier: DECISION TREE\n",
      "---------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------\n",
      "\t\tBefore Feature Selection\tAfter Feature Selection\n",
      "No. of features:\t 202 \t\t\t 15\n",
      "Train Accuracy:\t\t 100.0 \t\t\t 85.63\n",
      "Test Accuracy:\t\t 99.77 \t\t\t 85.02\n",
      "ROC AUC score:\t\t 1.0 \t\t\t 0.77\n",
      "f1_score:\t\t 1.0 \t\t\t 0.9\n",
      "Precision:\t\t 1.0 \t\t\t 0.88\n",
      "Recall:\t\t\t 1.0 \t\t\t 0.92\n",
      "MCC:\t\t\t 0.99 \t\t\t 0.57\n",
      "Train Time (in seconds): 6.93 \t\t\t 0.25\n",
      "Confusion Matrix(Before):\n",
      " [[1436    8]\n",
      " [   6 4612]]\n",
      "Confusion Matrix(after):\n",
      " [[ 885  559]\n",
      " [ 349 4269]]\n",
      "---------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Load dataset as pandas data frame\n",
    "filename = 'left_both.csv'\n",
    "dataset = pd.read_csv(filename)\n",
    "\n",
    "#Split data into input and output variable\n",
    "X = dataset.iloc[:,0:dataset.shape[1]-1]\n",
    "Y = dataset.iloc[:,-1]\n",
    "\n",
    "X_trainB, X_testB, y_trainB, y_testB = train_test_split(X, Y,test_size=0.3,random_state = 100)\n",
    "X_trainA, X_testA, y_trainA, y_testA = train_test_split(X, Y,test_size=0.3,random_state = 100)\n",
    "\n",
    "# Removing Constant features\n",
    "constant_filter = VarianceThreshold()\n",
    "constant_filter.fit(X_trainA)\n",
    "constant_columns = [col for col in X_trainA.columns\n",
    "                    if col not in X_trainA.columns[constant_filter.get_support()]]\n",
    "X_trainA.drop(labels=constant_columns,axis=1, inplace=True) \n",
    "X_testA.drop(labels=constant_columns,axis=1, inplace=True)\n",
    "\n",
    "# Removing Quasi-Constant features\n",
    "qconstant_filter = VarianceThreshold(0.01)\n",
    "qconstant_filter.fit(X_trainA)\n",
    "qconstant_columns = [col for col in X_trainA.columns\n",
    "                    if col not in X_trainA.columns[qconstant_filter.get_support()]]\n",
    "X_trainA.drop(labels=qconstant_columns,axis=1, inplace=True) \n",
    "X_testA.drop(labels=qconstant_columns,axis=1, inplace=True)\n",
    "\n",
    "# Removing Correlated Features\n",
    "correlated_features = set()  \n",
    "correlation_matrix = X_trainA.corr(method = 'pearson')\n",
    "for i in range(len(correlation_matrix.columns)):  \n",
    "    for j in range(i):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > 0.4:\n",
    "            colname = correlation_matrix.columns[i]\n",
    "            correlated_features.add(colname)\n",
    "X_trainA.drop(labels=correlated_features,axis=1, inplace=True) \n",
    "X_testA.drop(labels=correlated_features,axis=1, inplace=True)\n",
    "\n",
    "# feature ranking and selection    \n",
    "ranking_info, selected_features = feature_ranking_selection(X_trainA, y_trainA, 15)\n",
    "print('Fature Ranking information')\n",
    "print('---------------------------------------------------------------------------------------------')\n",
    "print(ranking_info)\n",
    "print(list(ranking_info['feature']))\n",
    "print('---------------------------------------------------------------------------------------------')\n",
    "X_trainA = X_trainA[selected_features]        \n",
    "X_testA = X_testA[selected_features] \n",
    "\n",
    "#names = [\"Nearest Neighbors\",\"Decision Tree\",\"Naive Bayes\"]\n",
    "names = [\"Decision Tree\"]\n",
    "\n",
    "classifiers = [\n",
    "    #KNeighborsClassifier(5, n_jobs= -1 ),\n",
    "    DecisionTreeClassifier(random_state = 100)\n",
    "    #GaussianNB(),\n",
    "   ]\n",
    "classifier2= [\n",
    "    #KNeighborsClassifier(5, n_jobs= -1 ),\n",
    "    DecisionTreeClassifier(max_depth = 5, min_samples_leaf = 50,random_state = 100)\n",
    "    #GaussianNB(),\n",
    "   ]\n",
    "clf_bef = list()\n",
    "clf_aft = list()\n",
    "\n",
    "for name, clf, dlf in zip(names,classifiers,classifier2):\n",
    "    # clf for before\n",
    "    # dlf for after\n",
    "    \n",
    "    # Before Feature Selection\n",
    "    startB = time.time()\n",
    "    clf.fit(X_trainB,y_trainB)    \n",
    "    endB = time.time() \n",
    "    clf_bef.append(clf)  \n",
    "    \n",
    "    # after Feature Selection    \n",
    "    startA = time.time()\n",
    "    dlf.fit(X_trainA,y_trainA)\n",
    "    endA = time.time()\n",
    "    clf_aft.append(dlf)\n",
    "        \n",
    "    print('\\t\\t\\t\\tClassifier:',name.upper())\n",
    "    print('---------------------------------------------------------------------------------------------')\n",
    "    print('---------------------------------------------------------------------------------------------')\n",
    "         \n",
    "    print('\\t\\tBefore Feature Selection\\tAfter Feature Selection')\n",
    "    print('No. of features:\\t', X_trainB.shape[1],'\\t\\t\\t',X_trainA.shape[1])\n",
    "    #print(\"Dataset Size(in MB):\\t\",(X_trainB.values.nbytes/1e6),'\\t\\t',(X_trainA.values.nbytes/1e6))\n",
    "\n",
    "    # training accuracy\n",
    "    train_predB = clf.predict(X_trainB)\n",
    "    train_predA = dlf.predict(X_trainA)\n",
    "    train_accB = round(accuracy_score(y_trainB,train_predB)*100, 2)\n",
    "    train_accA = round(accuracy_score(y_trainA,train_predA)*100, 2)\n",
    "    print('Train Accuracy:\\t\\t',train_accB,'\\t\\t\\t',train_accA)\n",
    "\n",
    "    # test accuracy \n",
    "    test_predB = clf.predict(X_testB)\n",
    "    test_predA = dlf.predict(X_testA)\n",
    "    test_accB = round(accuracy_score(y_testB,test_predB)*100, 2)\n",
    "    test_accA = round(accuracy_score(y_testA,test_predA)*100, 2)\n",
    "    print('Test Accuracy:\\t\\t',test_accB,'\\t\\t\\t',test_accA)\n",
    "        \n",
    "    # roc_auc_score\n",
    "    test_roc_aucB = round(roc_auc_score(y_testB,test_predB), 2)\n",
    "    test_roc_aucA = round(roc_auc_score(y_testA,test_predA), 2)\n",
    "    print('ROC AUC score:\\t\\t',test_roc_aucB,'\\t\\t\\t',test_roc_aucA)\n",
    "\n",
    "    # f1 score\n",
    "    test_f1B = round(f1_score(y_testB,test_predB),2)\n",
    "    test_f1A = round(f1_score(y_testA,test_predA),2)\n",
    "    print('f1_score:\\t\\t',test_f1B,'\\t\\t\\t',test_f1A)\n",
    "\n",
    "    # precision\n",
    "    test_precB = round(precision_score(y_testB,test_predB),2)\n",
    "    test_precA = round(precision_score(y_testA,test_predA),2)\n",
    "    print('Precision:\\t\\t',test_precB,'\\t\\t\\t',test_precA)\n",
    "\n",
    "    # recall\n",
    "    test_recallB = round(recall_score(y_testB,test_predB),2)\n",
    "    test_recallA = round(recall_score(y_testA,test_predA),2)\n",
    "    print('Recall:\\t\\t\\t',test_recallB,'\\t\\t\\t',test_recallA)\n",
    "\n",
    "    # Matthews correlation coefficient\n",
    "    test_MCCB = round(matthews_corrcoef(y_testB,test_predB),2)\n",
    "    test_MCCA = round(matthews_corrcoef(y_testA,test_predA),2)\n",
    "    print('MCC:\\t\\t\\t',test_MCCB,'\\t\\t\\t',test_MCCA)\n",
    "           \n",
    "    # training time\n",
    "    timeB = round((float(endB)- float(startB)),2)\n",
    "    timeA = round((float(endA)- float(startA)),2)\n",
    "    print('Train Time (in seconds):',timeB,'\\t\\t\\t',timeA)\n",
    "       \n",
    "    # confusion matrix\n",
    "    cm_resultB = confusion_matrix(y_testB,test_predB)\n",
    "    cm_resultA = confusion_matrix(y_testA,test_predA)\n",
    "    print('Confusion Matrix(Before):\\n',cm_resultB)\n",
    "    print('Confusion Matrix(after):\\n',cm_resultA)\n",
    "           \n",
    "    print('---------------------------------------------------------------------------------------------')\n",
    "    print('---------------------------------------------------------------------------------------------')\n",
    "    print('\\n')   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. R 202"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fature Ranking information\n",
      "---------------------------------------------------------------------------------------------\n",
      "              feature  importance\n",
      "0           t_mean_Ay    0.681472\n",
      "1            t_std_Gx    0.382075\n",
      "2   t_Crest_factor_Gy    0.313831\n",
      "3           t_mean_Ax    0.306845\n",
      "4   t_Crest_factor_Gz    0.225561\n",
      "5       t_kurtosis_Ax    0.213614\n",
      "6           t_mean_Az    0.211492\n",
      "7             f_Q1_Ay    0.167735\n",
      "8       t_kurtosis_Az    0.145834\n",
      "9       t_skewness_Gz    0.118359\n",
      "10      t_kurtosis_Gy    0.115324\n",
      "11          t_mean_Gz    0.104830\n",
      "12      t_skewness_Ax    0.080060\n",
      "13      f_variance_Gz    0.074364\n",
      "14      t_skewness_Ay    0.073285\n",
      "15      f_variance_Gx    0.051697\n",
      "16            f_Q1_Ax    0.048555\n",
      "17          t_mean_Gx    0.046560\n",
      "18      t_skewness_Gx    0.046246\n",
      "19          t_mean_Gy    0.038768\n",
      "20      t_kurtosis_Gx    0.036651\n",
      "21      f_variance_Gy    0.036031\n",
      "22      t_kurtosis_Gz    0.025429\n",
      "23      t_skewness_Az    0.019017\n",
      "24      t_skewness_Gy    0.002570\n",
      "['t_mean_Ay', 't_std_Gx', 't_Crest_factor_Gy', 't_mean_Ax', 't_Crest_factor_Gz', 't_kurtosis_Ax', 't_mean_Az', 'f_Q1_Ay', 't_kurtosis_Az', 't_skewness_Gz', 't_kurtosis_Gy', 't_mean_Gz', 't_skewness_Ax', 'f_variance_Gz', 't_skewness_Ay', 'f_variance_Gx', 'f_Q1_Ax', 't_mean_Gx', 't_skewness_Gx', 't_mean_Gy', 't_kurtosis_Gx', 'f_variance_Gy', 't_kurtosis_Gz', 't_skewness_Az', 't_skewness_Gy']\n",
      "---------------------------------------------------------------------------------------------\n",
      "\t\t\t\tClassifier: DECISION TREE\n",
      "---------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------\n",
      "\t\tBefore Feature Selection\tAfter Feature Selection\n",
      "No. of features:\t 202 \t\t\t 15\n",
      "Train Accuracy:\t\t 100.0 \t\t\t 99.24\n",
      "Test Accuracy:\t\t 99.59 \t\t\t 99.22\n",
      "ROC AUC score:\t\t 0.99 \t\t\t 0.99\n",
      "f1_score:\t\t 1.0 \t\t\t 0.99\n",
      "Precision:\t\t 1.0 \t\t\t 1.0\n",
      "Recall:\t\t\t 1.0 \t\t\t 0.99\n",
      "MCC:\t\t\t 0.99 \t\t\t 0.98\n",
      "Train Time (in seconds): 6.72 \t\t\t 0.23\n",
      "Confusion Matrix(Before):\n",
      " [[1479   16]\n",
      " [   9 4555]]\n",
      "Confusion Matrix(after):\n",
      " [[1474   21]\n",
      " [  26 4538]]\n",
      "---------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Load dataset as pandas data frame\n",
    "filename = 'right_both.csv'\n",
    "dataset = pd.read_csv(filename)\n",
    "\n",
    "#Split data into input and output variable\n",
    "X = dataset.iloc[:,0:dataset.shape[1]-1]\n",
    "Y = dataset.iloc[:,-1]\n",
    "\n",
    "X_trainB, X_testB, y_trainB, y_testB = train_test_split(X, Y,test_size=0.3,random_state = 100)\n",
    "X_trainA, X_testA, y_trainA, y_testA = train_test_split(X, Y,test_size=0.3,random_state = 100)\n",
    "\n",
    "# Removing Constant features\n",
    "constant_filter = VarianceThreshold()\n",
    "constant_filter.fit(X_trainA)\n",
    "constant_columns = [col for col in X_trainA.columns\n",
    "                    if col not in X_trainA.columns[constant_filter.get_support()]]\n",
    "X_trainA.drop(labels=constant_columns,axis=1, inplace=True) \n",
    "X_testA.drop(labels=constant_columns,axis=1, inplace=True)\n",
    "\n",
    "# Removing Quasi-Constant features\n",
    "qconstant_filter = VarianceThreshold(0.01)\n",
    "qconstant_filter.fit(X_trainA)\n",
    "qconstant_columns = [col for col in X_trainA.columns\n",
    "                    if col not in X_trainA.columns[qconstant_filter.get_support()]]\n",
    "X_trainA.drop(labels=qconstant_columns,axis=1, inplace=True) \n",
    "X_testA.drop(labels=qconstant_columns,axis=1, inplace=True)\n",
    "\n",
    "# Removing Correlated Features\n",
    "correlated_features = set()  \n",
    "correlation_matrix = X_trainA.corr(method = 'pearson')\n",
    "for i in range(len(correlation_matrix.columns)):  \n",
    "    for j in range(i):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > 0.4:\n",
    "            colname = correlation_matrix.columns[i]\n",
    "            correlated_features.add(colname)\n",
    "X_trainA.drop(labels=correlated_features,axis=1, inplace=True) \n",
    "X_testA.drop(labels=correlated_features,axis=1, inplace=True)\n",
    "\n",
    "# feature ranking and selection    \n",
    "ranking_info, selected_features = feature_ranking_selection(X_trainA, y_trainA, 15)\n",
    "print('Fature Ranking information')\n",
    "print('---------------------------------------------------------------------------------------------')\n",
    "print(ranking_info)\n",
    "print(list(ranking_info['feature']))\n",
    "print('---------------------------------------------------------------------------------------------')\n",
    "X_trainA = X_trainA[selected_features]        \n",
    "X_testA = X_testA[selected_features] \n",
    "\n",
    "#names = [\"Nearest Neighbors\",\"Decision Tree\",\"Naive Bayes\"]\n",
    "names = [\"Decision Tree\"]\n",
    "\n",
    "classifiers = [\n",
    "    #KNeighborsClassifier(5, n_jobs= -1 ),\n",
    "    DecisionTreeClassifier(random_state = 100)\n",
    "    #GaussianNB(),\n",
    "   ]\n",
    "classifier2= [\n",
    "    #KNeighborsClassifier(5, n_jobs= -1 ),\n",
    "    DecisionTreeClassifier(max_depth = 5, min_samples_leaf = 50,random_state = 100)\n",
    "    #GaussianNB(),\n",
    "   ]\n",
    "clf_bef = list()\n",
    "clf_aft = list()\n",
    "\n",
    "for name, clf, dlf in zip(names,classifiers,classifier2):\n",
    "    # clf for before\n",
    "    # dlf for after\n",
    "    \n",
    "    # Before Feature Selection\n",
    "    startB = time.time()\n",
    "    clf.fit(X_trainB,y_trainB)    \n",
    "    endB = time.time() \n",
    "    clf_bef.append(clf)  \n",
    "    \n",
    "    # after Feature Selection    \n",
    "    startA = time.time()\n",
    "    dlf.fit(X_trainA,y_trainA)\n",
    "    endA = time.time()\n",
    "    clf_aft.append(dlf)\n",
    "        \n",
    "    print('\\t\\t\\t\\tClassifier:',name.upper())\n",
    "    print('---------------------------------------------------------------------------------------------')\n",
    "    print('---------------------------------------------------------------------------------------------')\n",
    "         \n",
    "    print('\\t\\tBefore Feature Selection\\tAfter Feature Selection')\n",
    "    print('No. of features:\\t', X_trainB.shape[1],'\\t\\t\\t',X_trainA.shape[1])\n",
    "    #print(\"Dataset Size(in MB):\\t\",(X_trainB.values.nbytes/1e6),'\\t\\t',(X_trainA.values.nbytes/1e6))\n",
    "\n",
    "    # training accuracy\n",
    "    train_predB = clf.predict(X_trainB)\n",
    "    train_predA = dlf.predict(X_trainA)\n",
    "    train_accB = round(accuracy_score(y_trainB,train_predB)*100, 2)\n",
    "    train_accA = round(accuracy_score(y_trainA,train_predA)*100, 2)\n",
    "    print('Train Accuracy:\\t\\t',train_accB,'\\t\\t\\t',train_accA)\n",
    "\n",
    "    # test accuracy \n",
    "    test_predB = clf.predict(X_testB)\n",
    "    test_predA = dlf.predict(X_testA)\n",
    "    test_accB = round(accuracy_score(y_testB,test_predB)*100, 2)\n",
    "    test_accA = round(accuracy_score(y_testA,test_predA)*100, 2)\n",
    "    print('Test Accuracy:\\t\\t',test_accB,'\\t\\t\\t',test_accA)\n",
    "        \n",
    "    # roc_auc_score\n",
    "    test_roc_aucB = round(roc_auc_score(y_testB,test_predB), 2)\n",
    "    test_roc_aucA = round(roc_auc_score(y_testA,test_predA), 2)\n",
    "    print('ROC AUC score:\\t\\t',test_roc_aucB,'\\t\\t\\t',test_roc_aucA)\n",
    "\n",
    "    # f1 score\n",
    "    test_f1B = round(f1_score(y_testB,test_predB),2)\n",
    "    test_f1A = round(f1_score(y_testA,test_predA),2)\n",
    "    print('f1_score:\\t\\t',test_f1B,'\\t\\t\\t',test_f1A)\n",
    "\n",
    "    # precision\n",
    "    test_precB = round(precision_score(y_testB,test_predB),2)\n",
    "    test_precA = round(precision_score(y_testA,test_predA),2)\n",
    "    print('Precision:\\t\\t',test_precB,'\\t\\t\\t',test_precA)\n",
    "\n",
    "    # recall\n",
    "    test_recallB = round(recall_score(y_testB,test_predB),2)\n",
    "    test_recallA = round(recall_score(y_testA,test_predA),2)\n",
    "    print('Recall:\\t\\t\\t',test_recallB,'\\t\\t\\t',test_recallA)\n",
    "\n",
    "    # Matthews correlation coefficient\n",
    "    test_MCCB = round(matthews_corrcoef(y_testB,test_predB),2)\n",
    "    test_MCCA = round(matthews_corrcoef(y_testA,test_predA),2)\n",
    "    print('MCC:\\t\\t\\t',test_MCCB,'\\t\\t\\t',test_MCCA)\n",
    "           \n",
    "    # training time\n",
    "    timeB = round((float(endB)- float(startB)),2)\n",
    "    timeA = round((float(endA)- float(startA)),2)\n",
    "    print('Train Time (in seconds):',timeB,'\\t\\t\\t',timeA)\n",
    "       \n",
    "    # confusion matrix\n",
    "    cm_resultB = confusion_matrix(y_testB,test_predB)\n",
    "    cm_resultA = confusion_matrix(y_testA,test_predA)\n",
    "    print('Confusion Matrix(Before):\\n',cm_resultB)\n",
    "    print('Confusion Matrix(after):\\n',cm_resultA)\n",
    "           \n",
    "    print('---------------------------------------------------------------------------------------------')\n",
    "    print('---------------------------------------------------------------------------------------------')\n",
    "    print('\\n')      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. CL 202"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fature Ranking information\n",
      "---------------------------------------------------------------------------------------------\n",
      "              feature  importance\n",
      "0            t_std_Gx    0.364619\n",
      "1   t_Crest_factor_Gy    0.311454\n",
      "2   t_Crest_factor_Gz    0.293485\n",
      "3           t_mean_Az    0.234338\n",
      "4       t_kurtosis_Ay    0.209613\n",
      "5       t_kurtosis_Az    0.170695\n",
      "6       t_kurtosis_Ax    0.143661\n",
      "7       t_skewness_Ay    0.124805\n",
      "8             f_Q1_Ay    0.120271\n",
      "9           t_mean_Ay    0.105026\n",
      "10      t_kurtosis_Gy    0.091462\n",
      "11          t_mean_Ax    0.066357\n",
      "12      t_skewness_Gx    0.058520\n",
      "13          t_mean_Gy    0.056213\n",
      "14      t_skewness_Gz    0.053810\n",
      "15      f_variance_Gx    0.046632\n",
      "16  t_Crest_factor_Az    0.045637\n",
      "17      t_kurtosis_Gz    0.040732\n",
      "18          t_mean_Gz    0.039471\n",
      "19      t_skewness_Gy    0.028957\n",
      "20      t_skewness_Az    0.027161\n",
      "21          t_mean_Gx    0.020222\n",
      "22      t_kurtosis_Gx    0.018569\n",
      "23      t_skewness_Ax    0.007808\n",
      "24      f_variance_Gy    0.002943\n",
      "['t_std_Gx', 't_Crest_factor_Gy', 't_Crest_factor_Gz', 't_mean_Az', 't_kurtosis_Ay', 't_kurtosis_Az', 't_kurtosis_Ax', 't_skewness_Ay', 'f_Q1_Ay', 't_mean_Ay', 't_kurtosis_Gy', 't_mean_Ax', 't_skewness_Gx', 't_mean_Gy', 't_skewness_Gz', 'f_variance_Gx', 't_Crest_factor_Az', 't_kurtosis_Gz', 't_mean_Gz', 't_skewness_Gy', 't_skewness_Az', 't_mean_Gx', 't_kurtosis_Gx', 't_skewness_Ax', 'f_variance_Gy']\n",
      "---------------------------------------------------------------------------------------------\n",
      "\t\t\t\tClassifier: DECISION TREE\n",
      "---------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------\n",
      "\t\tBefore Feature Selection\tAfter Feature Selection\n",
      "No. of features:\t 202 \t\t\t 15\n",
      "Train Accuracy:\t\t 100.0 \t\t\t 99.21\n",
      "Test Accuracy:\t\t 99.88 \t\t\t 99.15\n",
      "ROC AUC score:\t\t 1.0 \t\t\t 0.99\n",
      "f1_score:\t\t 1.0 \t\t\t 0.99\n",
      "Precision:\t\t 1.0 \t\t\t 0.99\n",
      "Recall:\t\t\t 1.0 \t\t\t 1.0\n",
      "MCC:\t\t\t 1.0 \t\t\t 0.98\n",
      "Train Time (in seconds): 20.4 \t\t\t 0.56\n",
      "Confusion Matrix(Before):\n",
      " [[2996    8]\n",
      " [   7 9204]]\n",
      "Confusion Matrix(after):\n",
      " [[2936   68]\n",
      " [  36 9175]]\n",
      "---------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Load dataset as pandas data frame\n",
    "filename = 'CL_both_202.csv'\n",
    "dataset = pd.read_csv(filename)\n",
    "\n",
    "#Split data into input and output variable\n",
    "X = dataset.iloc[:,0:dataset.shape[1]-1]\n",
    "Y = dataset.iloc[:,-1]\n",
    "\n",
    "X_trainB, X_testB, y_trainB, y_testB = train_test_split(X, Y,test_size=0.3,random_state = 100)\n",
    "X_trainA, X_testA, y_trainA, y_testA = train_test_split(X, Y,test_size=0.3,random_state = 100)\n",
    "\n",
    "# Removing Constant features\n",
    "constant_filter = VarianceThreshold()\n",
    "constant_filter.fit(X_trainA)\n",
    "constant_columns = [col for col in X_trainA.columns\n",
    "                    if col not in X_trainA.columns[constant_filter.get_support()]]\n",
    "X_trainA.drop(labels=constant_columns,axis=1, inplace=True) \n",
    "X_testA.drop(labels=constant_columns,axis=1, inplace=True)\n",
    "\n",
    "# Removing Quasi-Constant features\n",
    "qconstant_filter = VarianceThreshold(0.01)\n",
    "qconstant_filter.fit(X_trainA)\n",
    "qconstant_columns = [col for col in X_trainA.columns\n",
    "                    if col not in X_trainA.columns[qconstant_filter.get_support()]]\n",
    "X_trainA.drop(labels=qconstant_columns,axis=1, inplace=True) \n",
    "X_testA.drop(labels=qconstant_columns,axis=1, inplace=True)\n",
    "\n",
    "# Removing Correlated Features\n",
    "correlated_features = set()  \n",
    "correlation_matrix = X_trainA.corr(method = 'pearson')\n",
    "for i in range(len(correlation_matrix.columns)):  \n",
    "    for j in range(i):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > 0.4:\n",
    "            colname = correlation_matrix.columns[i]\n",
    "            correlated_features.add(colname)\n",
    "X_trainA.drop(labels=correlated_features,axis=1, inplace=True) \n",
    "X_testA.drop(labels=correlated_features,axis=1, inplace=True)\n",
    "\n",
    "# feature ranking and selection    \n",
    "ranking_info, selected_features = feature_ranking_selection(X_trainA, y_trainA, 15)\n",
    "print('Fature Ranking information')\n",
    "print('---------------------------------------------------------------------------------------------')\n",
    "print(ranking_info)\n",
    "print(list(ranking_info['feature']))\n",
    "print('---------------------------------------------------------------------------------------------')\n",
    "X_trainA = X_trainA[selected_features]        \n",
    "X_testA = X_testA[selected_features] \n",
    "\n",
    "#names = [\"Nearest Neighbors\",\"Decision Tree\",\"Naive Bayes\"]\n",
    "names = [\"Decision Tree\"]\n",
    "\n",
    "classifiers = [\n",
    "    #KNeighborsClassifier(5, n_jobs= -1 ),\n",
    "    DecisionTreeClassifier(random_state = 100)\n",
    "    #GaussianNB(),\n",
    "   ]\n",
    "classifier2= [\n",
    "    #KNeighborsClassifier(5, n_jobs= -1 ),\n",
    "    DecisionTreeClassifier(max_depth = 5, min_samples_leaf = 50,random_state = 100)\n",
    "    #GaussianNB(),\n",
    "   ]\n",
    "clf_bef = list()\n",
    "clf_aft = list()\n",
    "\n",
    "for name, clf, dlf in zip(names,classifiers,classifier2):\n",
    "    # clf for before\n",
    "    # dlf for after\n",
    "    \n",
    "    # Before Feature Selection\n",
    "    startB = time.time()\n",
    "    clf.fit(X_trainB,y_trainB)    \n",
    "    endB = time.time() \n",
    "    clf_bef.append(clf)  \n",
    "    \n",
    "    # after Feature Selection    \n",
    "    startA = time.time()\n",
    "    dlf.fit(X_trainA,y_trainA)\n",
    "    endA = time.time()\n",
    "    clf_aft.append(dlf)\n",
    "        \n",
    "    print('\\t\\t\\t\\tClassifier:',name.upper())\n",
    "    print('---------------------------------------------------------------------------------------------')\n",
    "    print('---------------------------------------------------------------------------------------------')\n",
    "         \n",
    "    print('\\t\\tBefore Feature Selection\\tAfter Feature Selection')\n",
    "    print('No. of features:\\t', X_trainB.shape[1],'\\t\\t\\t',X_trainA.shape[1])\n",
    "    #print(\"Dataset Size(in MB):\\t\",(X_trainB.values.nbytes/1e6),'\\t\\t',(X_trainA.values.nbytes/1e6))\n",
    "\n",
    "    # training accuracy\n",
    "    train_predB = clf.predict(X_trainB)\n",
    "    train_predA = dlf.predict(X_trainA)\n",
    "    train_accB = round(accuracy_score(y_trainB,train_predB)*100, 2)\n",
    "    train_accA = round(accuracy_score(y_trainA,train_predA)*100, 2)\n",
    "    print('Train Accuracy:\\t\\t',train_accB,'\\t\\t\\t',train_accA)\n",
    "\n",
    "    # test accuracy \n",
    "    test_predB = clf.predict(X_testB)\n",
    "    test_predA = dlf.predict(X_testA)\n",
    "    test_accB = round(accuracy_score(y_testB,test_predB)*100, 2)\n",
    "    test_accA = round(accuracy_score(y_testA,test_predA)*100, 2)\n",
    "    print('Test Accuracy:\\t\\t',test_accB,'\\t\\t\\t',test_accA)\n",
    "        \n",
    "    # roc_auc_score\n",
    "    test_roc_aucB = round(roc_auc_score(y_testB,test_predB), 2)\n",
    "    test_roc_aucA = round(roc_auc_score(y_testA,test_predA), 2)\n",
    "    print('ROC AUC score:\\t\\t',test_roc_aucB,'\\t\\t\\t',test_roc_aucA)\n",
    "\n",
    "    # f1 score\n",
    "    test_f1B = round(f1_score(y_testB,test_predB),2)\n",
    "    test_f1A = round(f1_score(y_testA,test_predA),2)\n",
    "    print('f1_score:\\t\\t',test_f1B,'\\t\\t\\t',test_f1A)\n",
    "\n",
    "    # precision\n",
    "    test_precB = round(precision_score(y_testB,test_predB),2)\n",
    "    test_precA = round(precision_score(y_testA,test_predA),2)\n",
    "    print('Precision:\\t\\t',test_precB,'\\t\\t\\t',test_precA)\n",
    "\n",
    "    # recall\n",
    "    test_recallB = round(recall_score(y_testB,test_predB),2)\n",
    "    test_recallA = round(recall_score(y_testA,test_predA),2)\n",
    "    print('Recall:\\t\\t\\t',test_recallB,'\\t\\t\\t',test_recallA)\n",
    "\n",
    "    # Matthews correlation coefficient\n",
    "    test_MCCB = round(matthews_corrcoef(y_testB,test_predB),2)\n",
    "    test_MCCA = round(matthews_corrcoef(y_testA,test_predA),2)\n",
    "    print('MCC:\\t\\t\\t',test_MCCB,'\\t\\t\\t',test_MCCA)\n",
    "           \n",
    "    # training time\n",
    "    timeB = round((float(endB)- float(startB)),2)\n",
    "    timeA = round((float(endA)- float(startA)),2)\n",
    "    print('Train Time (in seconds):',timeB,'\\t\\t\\t',timeA)\n",
    "       \n",
    "    # confusion matrix\n",
    "    cm_resultB = confusion_matrix(y_testB,test_predB)\n",
    "    cm_resultA = confusion_matrix(y_testA,test_predA)\n",
    "    print('Confusion Matrix(Before):\\n',cm_resultB)\n",
    "    print('Confusion Matrix(after):\\n',cm_resultA)\n",
    "           \n",
    "    print('---------------------------------------------------------------------------------------------')\n",
    "    print('---------------------------------------------------------------------------------------------')\n",
    "    print('\\n')     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. CR 202"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fature Ranking information\n",
      "---------------------------------------------------------------------------------------------\n",
      "              feature  importance\n",
      "0           t_mean_Ay    0.757342\n",
      "1            t_std_Gx    0.365892\n",
      "2   t_Crest_factor_Gy    0.315936\n",
      "3   t_Crest_factor_Gz    0.298926\n",
      "4           t_mean_Az    0.238681\n",
      "5           t_mean_Ax    0.182010\n",
      "6       t_kurtosis_Az    0.171911\n",
      "7             f_Q1_Ay    0.171663\n",
      "8       t_kurtosis_Ax    0.155536\n",
      "9       t_skewness_Ay    0.132511\n",
      "10      t_skewness_Gx    0.091512\n",
      "11      t_kurtosis_Gy    0.090136\n",
      "12          t_mean_Gy    0.058150\n",
      "13      t_skewness_Gz    0.056084\n",
      "14          t_mean_Gz    0.054459\n",
      "15      t_kurtosis_Gz    0.050013\n",
      "16      t_skewness_Ax    0.043072\n",
      "17  t_Crest_factor_Az    0.041592\n",
      "18      f_variance_Gz    0.040691\n",
      "19            f_Q3_Ax    0.039053\n",
      "20      f_variance_Gx    0.030553\n",
      "21          t_mean_Gx    0.028659\n",
      "22      f_variance_Gy    0.027853\n",
      "23      t_skewness_Gy    0.019397\n",
      "24      t_kurtosis_Gx    0.016925\n",
      "25      t_skewness_Az    0.000378\n",
      "['t_mean_Ay', 't_std_Gx', 't_Crest_factor_Gy', 't_Crest_factor_Gz', 't_mean_Az', 't_mean_Ax', 't_kurtosis_Az', 'f_Q1_Ay', 't_kurtosis_Ax', 't_skewness_Ay', 't_skewness_Gx', 't_kurtosis_Gy', 't_mean_Gy', 't_skewness_Gz', 't_mean_Gz', 't_kurtosis_Gz', 't_skewness_Ax', 't_Crest_factor_Az', 'f_variance_Gz', 'f_Q3_Ax', 'f_variance_Gx', 't_mean_Gx', 'f_variance_Gy', 't_skewness_Gy', 't_kurtosis_Gx', 't_skewness_Az']\n",
      "---------------------------------------------------------------------------------------------\n",
      "\t\t\t\tClassifier: DECISION TREE\n",
      "---------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------\n",
      "\t\tBefore Feature Selection\tAfter Feature Selection\n",
      "No. of features:\t 202 \t\t\t 15\n",
      "Train Accuracy:\t\t 100.0 \t\t\t 99.41\n",
      "Test Accuracy:\t\t 99.85 \t\t\t 99.22\n",
      "ROC AUC score:\t\t 1.0 \t\t\t 0.99\n",
      "f1_score:\t\t 1.0 \t\t\t 0.99\n",
      "Precision:\t\t 1.0 \t\t\t 0.99\n",
      "Recall:\t\t\t 1.0 \t\t\t 1.0\n",
      "MCC:\t\t\t 1.0 \t\t\t 0.98\n",
      "Train Time (in seconds): 21.17 \t\t\t 0.51\n",
      "Confusion Matrix(Before):\n",
      " [[3047   10]\n",
      " [   8 9147]]\n",
      "Confusion Matrix(after):\n",
      " [[2995   62]\n",
      " [  33 9122]]\n",
      "---------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Load dataset as pandas data frame\n",
    "filename = 'CR_both_202.csv'\n",
    "dataset = pd.read_csv(filename)\n",
    "\n",
    "#Split data into input and output variable\n",
    "X = dataset.iloc[:,0:dataset.shape[1]-1]\n",
    "Y = dataset.iloc[:,-1]\n",
    "\n",
    "X_trainB, X_testB, y_trainB, y_testB = train_test_split(X, Y,test_size=0.3,random_state = 100)\n",
    "X_trainA, X_testA, y_trainA, y_testA = train_test_split(X, Y,test_size=0.3,random_state = 100)\n",
    "\n",
    "# Removing Constant features\n",
    "constant_filter = VarianceThreshold()\n",
    "constant_filter.fit(X_trainA)\n",
    "constant_columns = [col for col in X_trainA.columns\n",
    "                    if col not in X_trainA.columns[constant_filter.get_support()]]\n",
    "X_trainA.drop(labels=constant_columns,axis=1, inplace=True) \n",
    "X_testA.drop(labels=constant_columns,axis=1, inplace=True)\n",
    "\n",
    "# Removing Quasi-Constant features\n",
    "qconstant_filter = VarianceThreshold(0.01)\n",
    "qconstant_filter.fit(X_trainA)\n",
    "qconstant_columns = [col for col in X_trainA.columns\n",
    "                    if col not in X_trainA.columns[qconstant_filter.get_support()]]\n",
    "X_trainA.drop(labels=qconstant_columns,axis=1, inplace=True) \n",
    "X_testA.drop(labels=qconstant_columns,axis=1, inplace=True)\n",
    "\n",
    "# Removing Correlated Features\n",
    "correlated_features = set()  \n",
    "correlation_matrix = X_trainA.corr(method = 'pearson')\n",
    "for i in range(len(correlation_matrix.columns)):  \n",
    "    for j in range(i):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > 0.4:\n",
    "            colname = correlation_matrix.columns[i]\n",
    "            correlated_features.add(colname)\n",
    "X_trainA.drop(labels=correlated_features,axis=1, inplace=True) \n",
    "X_testA.drop(labels=correlated_features,axis=1, inplace=True)\n",
    "\n",
    "# feature ranking and selection    \n",
    "ranking_info, selected_features = feature_ranking_selection(X_trainA, y_trainA, 15)\n",
    "print('Fature Ranking information')\n",
    "print('---------------------------------------------------------------------------------------------')\n",
    "print(ranking_info)\n",
    "print(list(ranking_info['feature']))\n",
    "print('---------------------------------------------------------------------------------------------')\n",
    "X_trainA = X_trainA[selected_features]        \n",
    "X_testA = X_testA[selected_features] \n",
    "\n",
    "#names = [\"Nearest Neighbors\",\"Decision Tree\",\"Naive Bayes\"]\n",
    "names = [\"Decision Tree\"]\n",
    "\n",
    "classifiers = [\n",
    "    #KNeighborsClassifier(5, n_jobs= -1 ),\n",
    "    DecisionTreeClassifier(random_state = 100)\n",
    "    #GaussianNB(),\n",
    "   ]\n",
    "classifier2= [\n",
    "    #KNeighborsClassifier(5, n_jobs= -1 ),\n",
    "    DecisionTreeClassifier(max_depth = 5, min_samples_leaf = 50,random_state = 100)\n",
    "    #GaussianNB(),\n",
    "   ]\n",
    "clf_bef = list()\n",
    "clf_aft = list()\n",
    "\n",
    "for name, clf, dlf in zip(names,classifiers,classifier2):\n",
    "    # clf for before\n",
    "    # dlf for after\n",
    "    \n",
    "    # Before Feature Selection\n",
    "    startB = time.time()\n",
    "    clf.fit(X_trainB,y_trainB)    \n",
    "    endB = time.time() \n",
    "    clf_bef.append(clf)  \n",
    "    \n",
    "    # after Feature Selection    \n",
    "    startA = time.time()\n",
    "    dlf.fit(X_trainA,y_trainA)\n",
    "    endA = time.time()\n",
    "    clf_aft.append(dlf)\n",
    "        \n",
    "    print('\\t\\t\\t\\tClassifier:',name.upper())\n",
    "    print('---------------------------------------------------------------------------------------------')\n",
    "    print('---------------------------------------------------------------------------------------------')\n",
    "         \n",
    "    print('\\t\\tBefore Feature Selection\\tAfter Feature Selection')\n",
    "    print('No. of features:\\t', X_trainB.shape[1],'\\t\\t\\t',X_trainA.shape[1])\n",
    "    #print(\"Dataset Size(in MB):\\t\",(X_trainB.values.nbytes/1e6),'\\t\\t',(X_trainA.values.nbytes/1e6))\n",
    "\n",
    "    # training accuracy\n",
    "    train_predB = clf.predict(X_trainB)\n",
    "    train_predA = dlf.predict(X_trainA)\n",
    "    train_accB = round(accuracy_score(y_trainB,train_predB)*100, 2)\n",
    "    train_accA = round(accuracy_score(y_trainA,train_predA)*100, 2)\n",
    "    print('Train Accuracy:\\t\\t',train_accB,'\\t\\t\\t',train_accA)\n",
    "\n",
    "    # test accuracy \n",
    "    test_predB = clf.predict(X_testB)\n",
    "    test_predA = dlf.predict(X_testA)\n",
    "    test_accB = round(accuracy_score(y_testB,test_predB)*100, 2)\n",
    "    test_accA = round(accuracy_score(y_testA,test_predA)*100, 2)\n",
    "    print('Test Accuracy:\\t\\t',test_accB,'\\t\\t\\t',test_accA)\n",
    "        \n",
    "    # roc_auc_score\n",
    "    test_roc_aucB = round(roc_auc_score(y_testB,test_predB), 2)\n",
    "    test_roc_aucA = round(roc_auc_score(y_testA,test_predA), 2)\n",
    "    print('ROC AUC score:\\t\\t',test_roc_aucB,'\\t\\t\\t',test_roc_aucA)\n",
    "\n",
    "    # f1 score\n",
    "    test_f1B = round(f1_score(y_testB,test_predB),2)\n",
    "    test_f1A = round(f1_score(y_testA,test_predA),2)\n",
    "    print('f1_score:\\t\\t',test_f1B,'\\t\\t\\t',test_f1A)\n",
    "\n",
    "    # precision\n",
    "    test_precB = round(precision_score(y_testB,test_predB),2)\n",
    "    test_precA = round(precision_score(y_testA,test_predA),2)\n",
    "    print('Precision:\\t\\t',test_precB,'\\t\\t\\t',test_precA)\n",
    "\n",
    "    # recall\n",
    "    test_recallB = round(recall_score(y_testB,test_predB),2)\n",
    "    test_recallA = round(recall_score(y_testA,test_predA),2)\n",
    "    print('Recall:\\t\\t\\t',test_recallB,'\\t\\t\\t',test_recallA)\n",
    "\n",
    "    # Matthews correlation coefficient\n",
    "    test_MCCB = round(matthews_corrcoef(y_testB,test_predB),2)\n",
    "    test_MCCA = round(matthews_corrcoef(y_testA,test_predA),2)\n",
    "    print('MCC:\\t\\t\\t',test_MCCB,'\\t\\t\\t',test_MCCA)\n",
    "           \n",
    "    # training time\n",
    "    timeB = round((float(endB)- float(startB)),2)\n",
    "    timeA = round((float(endA)- float(startA)),2)\n",
    "    print('Train Time (in seconds):',timeB,'\\t\\t\\t',timeA)\n",
    "       \n",
    "    # confusion matrix\n",
    "    cm_resultB = confusion_matrix(y_testB,test_predB)\n",
    "    cm_resultA = confusion_matrix(y_testA,test_predA)\n",
    "    print('Confusion Matrix(Before):\\n',cm_resultB)\n",
    "    print('Confusion Matrix(after):\\n',cm_resultA)\n",
    "           \n",
    "    print('---------------------------------------------------------------------------------------------')\n",
    "    print('---------------------------------------------------------------------------------------------')\n",
    "    print('\\n')      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. LR 202"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fature Ranking information\n",
      "---------------------------------------------------------------------------------------------\n",
      "              feature  importance\n",
      "0            t_std_Gx    0.379937\n",
      "1   t_Crest_factor_Gy    0.300106\n",
      "2   t_Crest_factor_Gz    0.221806\n",
      "3       t_kurtosis_Ax    0.212675\n",
      "4           t_mean_Az    0.205662\n",
      "5       t_kurtosis_Ay    0.167969\n",
      "6       t_kurtosis_Az    0.140898\n",
      "7       t_kurtosis_Gy    0.115344\n",
      "8           t_mean_Ay    0.110007\n",
      "9             f_Q1_Ay    0.100507\n",
      "10          t_mean_Ax    0.080866\n",
      "11      t_skewness_Ay    0.066386\n",
      "12      t_skewness_Az    0.052049\n",
      "13          t_mean_Gx    0.044216\n",
      "14      t_kurtosis_Gx    0.042022\n",
      "15      t_skewness_Ax    0.038296\n",
      "16            f_Q1_Ax    0.037819\n",
      "17          t_mean_Gy    0.036306\n",
      "18      f_variance_Gx    0.031432\n",
      "19      t_kurtosis_Gz    0.030132\n",
      "20      t_skewness_Gx    0.023294\n",
      "21      f_variance_Gy    0.015630\n",
      "22      f_variance_Gz    0.012267\n",
      "23      t_skewness_Gy    0.010211\n",
      "24      t_skewness_Gz    0.001545\n",
      "25          t_mean_Gz    0.001075\n",
      "['t_std_Gx', 't_Crest_factor_Gy', 't_Crest_factor_Gz', 't_kurtosis_Ax', 't_mean_Az', 't_kurtosis_Ay', 't_kurtosis_Az', 't_kurtosis_Gy', 't_mean_Ay', 'f_Q1_Ay', 't_mean_Ax', 't_skewness_Ay', 't_skewness_Az', 't_mean_Gx', 't_kurtosis_Gx', 't_skewness_Ax', 'f_Q1_Ax', 't_mean_Gy', 'f_variance_Gx', 't_kurtosis_Gz', 't_skewness_Gx', 'f_variance_Gy', 'f_variance_Gz', 't_skewness_Gy', 't_skewness_Gz', 't_mean_Gz']\n",
      "---------------------------------------------------------------------------------------------\n",
      "\t\t\t\tClassifier: DECISION TREE\n",
      "---------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------\n",
      "\t\tBefore Feature Selection\tAfter Feature Selection\n",
      "No. of features:\t 202 \t\t\t 15\n",
      "Train Accuracy:\t\t 100.0 \t\t\t 99.22\n",
      "Test Accuracy:\t\t 99.85 \t\t\t 99.13\n",
      "ROC AUC score:\t\t 1.0 \t\t\t 0.99\n",
      "f1_score:\t\t 1.0 \t\t\t 0.99\n",
      "Precision:\t\t 1.0 \t\t\t 0.99\n",
      "Recall:\t\t\t 1.0 \t\t\t 1.0\n",
      "MCC:\t\t\t 1.0 \t\t\t 0.98\n",
      "Train Time (in seconds): 18.81 \t\t\t 0.55\n",
      "Confusion Matrix(Before):\n",
      " [[2905   10]\n",
      " [   8 9198]]\n",
      "Confusion Matrix(after):\n",
      " [[2854   61]\n",
      " [  45 9161]]\n",
      "---------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Load dataset as pandas data frame\n",
    "filename = 'LR_both_202.csv'\n",
    "dataset = pd.read_csv(filename)\n",
    "\n",
    "#Split data into input and output variable\n",
    "X = dataset.iloc[:,0:dataset.shape[1]-1]\n",
    "Y = dataset.iloc[:,-1]\n",
    "\n",
    "X_trainB, X_testB, y_trainB, y_testB = train_test_split(X, Y,test_size=0.3,random_state = 100)\n",
    "X_trainA, X_testA, y_trainA, y_testA = train_test_split(X, Y,test_size=0.3,random_state = 100)\n",
    "\n",
    "# Removing Constant features\n",
    "constant_filter = VarianceThreshold()\n",
    "constant_filter.fit(X_trainA)\n",
    "constant_columns = [col for col in X_trainA.columns\n",
    "                    if col not in X_trainA.columns[constant_filter.get_support()]]\n",
    "X_trainA.drop(labels=constant_columns,axis=1, inplace=True) \n",
    "X_testA.drop(labels=constant_columns,axis=1, inplace=True)\n",
    "\n",
    "# Removing Quasi-Constant features\n",
    "qconstant_filter = VarianceThreshold(0.01)\n",
    "qconstant_filter.fit(X_trainA)\n",
    "qconstant_columns = [col for col in X_trainA.columns\n",
    "                    if col not in X_trainA.columns[qconstant_filter.get_support()]]\n",
    "X_trainA.drop(labels=qconstant_columns,axis=1, inplace=True) \n",
    "X_testA.drop(labels=qconstant_columns,axis=1, inplace=True)\n",
    "\n",
    "# Removing Correlated Features\n",
    "correlated_features = set()  \n",
    "correlation_matrix = X_trainA.corr(method = 'pearson')\n",
    "for i in range(len(correlation_matrix.columns)):  \n",
    "    for j in range(i):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > 0.4:\n",
    "            colname = correlation_matrix.columns[i]\n",
    "            correlated_features.add(colname)\n",
    "X_trainA.drop(labels=correlated_features,axis=1, inplace=True) \n",
    "X_testA.drop(labels=correlated_features,axis=1, inplace=True)\n",
    "\n",
    "# feature ranking and selection    \n",
    "ranking_info, selected_features = feature_ranking_selection(X_trainA, y_trainA, 15)\n",
    "print('Fature Ranking information')\n",
    "print('---------------------------------------------------------------------------------------------')\n",
    "print(ranking_info)\n",
    "print(list(ranking_info['feature']))\n",
    "print('---------------------------------------------------------------------------------------------')\n",
    "X_trainA = X_trainA[selected_features]        \n",
    "X_testA = X_testA[selected_features] \n",
    "\n",
    "#names = [\"Nearest Neighbors\",\"Decision Tree\",\"Naive Bayes\"]\n",
    "names = [\"Decision Tree\"]\n",
    "\n",
    "classifiers = [\n",
    "    #KNeighborsClassifier(5, n_jobs= -1 ),\n",
    "    DecisionTreeClassifier(random_state = 100)\n",
    "    #GaussianNB(),\n",
    "   ]\n",
    "classifier2= [\n",
    "    #KNeighborsClassifier(5, n_jobs= -1 ),\n",
    "    DecisionTreeClassifier(max_depth = 5, min_samples_leaf = 50,random_state = 100)\n",
    "    #GaussianNB(),\n",
    "   ]\n",
    "clf_bef = list()\n",
    "clf_aft = list()\n",
    "\n",
    "for name, clf, dlf in zip(names,classifiers,classifier2):\n",
    "    # clf for before\n",
    "    # dlf for after\n",
    "    \n",
    "    # Before Feature Selection\n",
    "    startB = time.time()\n",
    "    clf.fit(X_trainB,y_trainB)    \n",
    "    endB = time.time() \n",
    "    clf_bef.append(clf)  \n",
    "    \n",
    "    # after Feature Selection    \n",
    "    startA = time.time()\n",
    "    dlf.fit(X_trainA,y_trainA)\n",
    "    endA = time.time()\n",
    "    clf_aft.append(dlf)\n",
    "        \n",
    "    print('\\t\\t\\t\\tClassifier:',name.upper())\n",
    "    print('---------------------------------------------------------------------------------------------')\n",
    "    print('---------------------------------------------------------------------------------------------')\n",
    "         \n",
    "    print('\\t\\tBefore Feature Selection\\tAfter Feature Selection')\n",
    "    print('No. of features:\\t', X_trainB.shape[1],'\\t\\t\\t',X_trainA.shape[1])\n",
    "    #print(\"Dataset Size(in MB):\\t\",(X_trainB.values.nbytes/1e6),'\\t\\t',(X_trainA.values.nbytes/1e6))\n",
    "\n",
    "    # training accuracy\n",
    "    train_predB = clf.predict(X_trainB)\n",
    "    train_predA = dlf.predict(X_trainA)\n",
    "    train_accB = round(accuracy_score(y_trainB,train_predB)*100, 2)\n",
    "    train_accA = round(accuracy_score(y_trainA,train_predA)*100, 2)\n",
    "    print('Train Accuracy:\\t\\t',train_accB,'\\t\\t\\t',train_accA)\n",
    "\n",
    "    # test accuracy \n",
    "    test_predB = clf.predict(X_testB)\n",
    "    test_predA = dlf.predict(X_testA)\n",
    "    test_accB = round(accuracy_score(y_testB,test_predB)*100, 2)\n",
    "    test_accA = round(accuracy_score(y_testA,test_predA)*100, 2)\n",
    "    print('Test Accuracy:\\t\\t',test_accB,'\\t\\t\\t',test_accA)\n",
    "        \n",
    "    # roc_auc_score\n",
    "    test_roc_aucB = round(roc_auc_score(y_testB,test_predB), 2)\n",
    "    test_roc_aucA = round(roc_auc_score(y_testA,test_predA), 2)\n",
    "    print('ROC AUC score:\\t\\t',test_roc_aucB,'\\t\\t\\t',test_roc_aucA)\n",
    "\n",
    "    # f1 score\n",
    "    test_f1B = round(f1_score(y_testB,test_predB),2)\n",
    "    test_f1A = round(f1_score(y_testA,test_predA),2)\n",
    "    print('f1_score:\\t\\t',test_f1B,'\\t\\t\\t',test_f1A)\n",
    "\n",
    "    # precision\n",
    "    test_precB = round(precision_score(y_testB,test_predB),2)\n",
    "    test_precA = round(precision_score(y_testA,test_predA),2)\n",
    "    print('Precision:\\t\\t',test_precB,'\\t\\t\\t',test_precA)\n",
    "\n",
    "    # recall\n",
    "    test_recallB = round(recall_score(y_testB,test_predB),2)\n",
    "    test_recallA = round(recall_score(y_testA,test_predA),2)\n",
    "    print('Recall:\\t\\t\\t',test_recallB,'\\t\\t\\t',test_recallA)\n",
    "\n",
    "    # Matthews correlation coefficient\n",
    "    test_MCCB = round(matthews_corrcoef(y_testB,test_predB),2)\n",
    "    test_MCCA = round(matthews_corrcoef(y_testA,test_predA),2)\n",
    "    print('MCC:\\t\\t\\t',test_MCCB,'\\t\\t\\t',test_MCCA)\n",
    "           \n",
    "    # training time\n",
    "    timeB = round((float(endB)- float(startB)),2)\n",
    "    timeA = round((float(endA)- float(startA)),2)\n",
    "    print('Train Time (in seconds):',timeB,'\\t\\t\\t',timeA)\n",
    "       \n",
    "    # confusion matrix\n",
    "    cm_resultB = confusion_matrix(y_testB,test_predB)\n",
    "    cm_resultA = confusion_matrix(y_testA,test_predA)\n",
    "    print('Confusion Matrix(Before):\\n',cm_resultB)\n",
    "    print('Confusion Matrix(after):\\n',cm_resultA)\n",
    "           \n",
    "    print('---------------------------------------------------------------------------------------------')\n",
    "    print('---------------------------------------------------------------------------------------------')\n",
    "    print('\\n')     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. CLR 606"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fature Ranking information\n",
      "---------------------------------------------------------------------------------------------\n",
      "            feature  importance\n",
      "0       R_t_mean_Ax    0.061716\n",
      "1         C_f_Q1_Ay    0.045072\n",
      "2       C_t_mean_Gx    0.042654\n",
      "3       R_t_mean_Ay    0.032141\n",
      "4       L_t_mean_Gz    0.031806\n",
      "5        L_t_std_Gx    0.029716\n",
      "6   C_t_variance_Ax    0.027469\n",
      "7       R_t_mean_Gz    0.026112\n",
      "8   C_t_skewness_Az    0.025879\n",
      "9        L_t_std_Gy    0.018739\n",
      "10  R_t_skewness_Az    0.017907\n",
      "11      L_t_mean_Ax    0.017483\n",
      "12      C_t_mean_Az    0.017329\n",
      "13  R_t_skewness_Gz    0.017088\n",
      "14       C_f_IQR_Az    0.016851\n",
      "15  C_t_variance_Ay    0.016844\n",
      "16  R_t_skewness_Gy    0.016700\n",
      "17       C_t_std_Gx    0.016542\n",
      "18  C_t_kurtosis_Gz    0.015260\n",
      "19  L_f_variance_Gz    0.015008\n",
      "20  L_t_skewness_Gx    0.014600\n",
      "21  R_t_kurtosis_Gx    0.014318\n",
      "22      L_t_mean_Az    0.013661\n",
      "23  R_t_kurtosis_Ay    0.013387\n",
      "24  C_t_kurtosis_Az    0.013190\n",
      "25      R_t_mean_Gy    0.012960\n",
      "26  R_t_kurtosis_Az    0.012431\n",
      "27      R_t_mean_Az    0.012428\n",
      "28  R_t_kurtosis_Gy    0.011822\n",
      "29      C_f_mean_Ay    0.011498\n",
      "..              ...         ...\n",
      "40  C_t_kurtosis_Gy    0.008095\n",
      "41  R_t_kurtosis_Ax    0.007924\n",
      "42        R_f_Q1_Ay    0.007865\n",
      "43      C_t_mean_Gz    0.006846\n",
      "44  C_t_skewness_Gz    0.006537\n",
      "45  C_f_variance_Gx    0.006485\n",
      "46  R_t_kurtosis_Gz    0.005783\n",
      "47        L_f_Q1_Ax    0.005683\n",
      "48        R_f_Q1_Ax    0.005639\n",
      "49  L_t_skewness_Gz    0.005487\n",
      "50  C_t_kurtosis_Ay    0.005408\n",
      "51      C_f_mean_Gz    0.004924\n",
      "52  C_t_kurtosis_Gx    0.004684\n",
      "53  R_t_skewness_Ax    0.004580\n",
      "54  L_t_kurtosis_Gz    0.003943\n",
      "55       R_t_std_Gx    0.003849\n",
      "56  R_t_skewness_Ay    0.003236\n",
      "57      L_t_mean_Gx    0.003108\n",
      "58  L_f_variance_Gx    0.002790\n",
      "59  C_f_variance_Gy    0.002163\n",
      "60      R_t_mean_Gx    0.001971\n",
      "61        L_f_Q1_Ay    0.001870\n",
      "62  R_f_variance_Gy    0.001126\n",
      "63  C_t_skewness_Ax    0.001116\n",
      "64  L_t_kurtosis_Gx    0.001067\n",
      "65  L_t_kurtosis_Ay    0.000740\n",
      "66  L_f_variance_Gy    0.000708\n",
      "67  C_t_skewness_Gy    0.000572\n",
      "68  L_t_skewness_Gy    0.000492\n",
      "69  C_f_kurtosis_Ax    0.000410\n",
      "\n",
      "[70 rows x 2 columns]\n",
      "['R_t_mean_Ax', 'C_f_Q1_Ay', 'C_t_mean_Gx', 'R_t_mean_Ay', 'L_t_mean_Gz', 'L_t_std_Gx', 'C_t_variance_Ax', 'R_t_mean_Gz', 'C_t_skewness_Az', 'L_t_std_Gy', 'R_t_skewness_Az', 'L_t_mean_Ax', 'C_t_mean_Az', 'R_t_skewness_Gz', 'C_f_IQR_Az', 'C_t_variance_Ay', 'R_t_skewness_Gy', 'C_t_std_Gx', 'C_t_kurtosis_Gz', 'L_f_variance_Gz', 'L_t_skewness_Gx', 'R_t_kurtosis_Gx', 'L_t_mean_Az', 'R_t_kurtosis_Ay', 'C_t_kurtosis_Az', 'R_t_mean_Gy', 'R_t_kurtosis_Az', 'R_t_mean_Az', 'R_t_kurtosis_Gy', 'C_f_mean_Ay', 'L_t_kurtosis_Gy', 'C_t_mean_Gy', 'C_t_kurtosis_Ax', 'R_f_variance_Gz', 'R_t_std_Gy', 'L_t_kurtosis_Az', 'L_t_kurtosis_Ax', 'L_t_mean_Gy', 'L_t_skewness_Az', 'R_f_variance_Gx', 'C_t_kurtosis_Gy', 'R_t_kurtosis_Ax', 'R_f_Q1_Ay', 'C_t_mean_Gz', 'C_t_skewness_Gz', 'C_f_variance_Gx', 'R_t_kurtosis_Gz', 'L_f_Q1_Ax', 'R_f_Q1_Ax', 'L_t_skewness_Gz', 'C_t_kurtosis_Ay', 'C_f_mean_Gz', 'C_t_kurtosis_Gx', 'R_t_skewness_Ax', 'L_t_kurtosis_Gz', 'R_t_std_Gx', 'R_t_skewness_Ay', 'L_t_mean_Gx', 'L_f_variance_Gx', 'C_f_variance_Gy', 'R_t_mean_Gx', 'L_f_Q1_Ay', 'R_f_variance_Gy', 'C_t_skewness_Ax', 'L_t_kurtosis_Gx', 'L_t_kurtosis_Ay', 'L_f_variance_Gy', 'C_t_skewness_Gy', 'L_t_skewness_Gy', 'C_f_kurtosis_Ax']\n",
      "---------------------------------------------------------------------------------------------\n",
      "\t\t\t\tClassifier: DECISION TREE\n",
      "---------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------\n",
      "\t\tBefore Feature Selection\tAfter Feature Selection\n",
      "No. of features:\t 606 \t\t\t 15\n",
      "Train Accuracy:\t\t 83.41 \t\t\t 76.21\n",
      "Test Accuracy:\t\t 46.42 \t\t\t 75.19\n",
      "ROC AUC score:\t\t 0.34 \t\t\t 0.5\n",
      "f1_score:\t\t 0.62 \t\t\t 0.86\n",
      "Precision:\t\t 0.66 \t\t\t 0.75\n",
      "Recall:\t\t\t 0.58 \t\t\t 1.0\n",
      "MCC:\t\t\t -0.29 \t\t\t 0.0\n",
      "Train Time (in seconds): 26.85 \t\t\t 0.21\n",
      "Confusion Matrix(Before):\n",
      " [[ 155 1344]\n",
      " [1894 2650]]\n",
      "Confusion Matrix(after):\n",
      " [[   0 1499]\n",
      " [   0 4544]]\n",
      "---------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Load dataset as pandas data frame\n",
    "filename = 'CLR_both_606.csv'\n",
    "dataset = pd.read_csv(filename)\n",
    "\n",
    "#Split data into input and output variable\n",
    "X = dataset.iloc[:,0:dataset.shape[1]-1]\n",
    "Y = dataset.iloc[:,-1]\n",
    "\n",
    "X_trainB, X_testB, y_trainB, y_testB = train_test_split(X, Y,test_size=0.3,random_state = 100)\n",
    "X_trainA, X_testA, y_trainA, y_testA = train_test_split(X, Y,test_size=0.3,random_state = 100)\n",
    "\n",
    "# Removing Constant features\n",
    "constant_filter = VarianceThreshold()\n",
    "constant_filter.fit(X_trainA)\n",
    "constant_columns = [col for col in X_trainA.columns\n",
    "                    if col not in X_trainA.columns[constant_filter.get_support()]]\n",
    "X_trainA.drop(labels=constant_columns,axis=1, inplace=True) \n",
    "X_testA.drop(labels=constant_columns,axis=1, inplace=True)\n",
    "\n",
    "# Removing Quasi-Constant features\n",
    "qconstant_filter = VarianceThreshold(0.01)\n",
    "qconstant_filter.fit(X_trainA)\n",
    "qconstant_columns = [col for col in X_trainA.columns\n",
    "                    if col not in X_trainA.columns[qconstant_filter.get_support()]]\n",
    "X_trainA.drop(labels=qconstant_columns,axis=1, inplace=True) \n",
    "X_testA.drop(labels=qconstant_columns,axis=1, inplace=True)\n",
    "\n",
    "# Removing Correlated Features\n",
    "correlated_features = set()  \n",
    "correlation_matrix = X_trainA.corr(method = 'pearson')\n",
    "for i in range(len(correlation_matrix.columns)):  \n",
    "    for j in range(i):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > 0.4:\n",
    "            colname = correlation_matrix.columns[i]\n",
    "            correlated_features.add(colname)\n",
    "X_trainA.drop(labels=correlated_features,axis=1, inplace=True) \n",
    "X_testA.drop(labels=correlated_features,axis=1, inplace=True)\n",
    "\n",
    "# feature ranking and selection    \n",
    "ranking_info, selected_features = feature_ranking_selection(X_trainA, y_trainA, 15)\n",
    "print('Fature Ranking information')\n",
    "print('---------------------------------------------------------------------------------------------')\n",
    "print(ranking_info)\n",
    "print(list(ranking_info['feature']))\n",
    "print('---------------------------------------------------------------------------------------------')\n",
    "X_trainA = X_trainA[selected_features]        \n",
    "X_testA = X_testA[selected_features] \n",
    "\n",
    "#names = [\"Nearest Neighbors\",\"Decision Tree\",\"Naive Bayes\"]\n",
    "names = [\"Decision Tree\"]\n",
    "\n",
    "classifiers = [\n",
    "    #KNeighborsClassifier(5, n_jobs= -1 ),\n",
    "    DecisionTreeClassifier(random_state = 100)\n",
    "    #GaussianNB(),\n",
    "   ]\n",
    "classifier2= [\n",
    "    #KNeighborsClassifier(5, n_jobs= -1 ),\n",
    "    DecisionTreeClassifier(max_depth = 5, min_samples_leaf = 50,random_state = 100)\n",
    "    #GaussianNB(),\n",
    "   ]\n",
    "clf_bef = list()\n",
    "clf_aft = list()\n",
    "\n",
    "for name, clf, dlf in zip(names,classifiers,classifier2):\n",
    "    # clf for before\n",
    "    # dlf for after\n",
    "    \n",
    "    # Before Feature Selection\n",
    "    startB = time.time()\n",
    "    clf.fit(X_trainB,y_trainB)    \n",
    "    endB = time.time() \n",
    "    clf_bef.append(clf)  \n",
    "    \n",
    "    # after Feature Selection    \n",
    "    startA = time.time()\n",
    "    dlf.fit(X_trainA,y_trainA)\n",
    "    endA = time.time()\n",
    "    clf_aft.append(dlf)\n",
    "        \n",
    "    print('\\t\\t\\t\\tClassifier:',name.upper())\n",
    "    print('---------------------------------------------------------------------------------------------')\n",
    "    print('---------------------------------------------------------------------------------------------')\n",
    "         \n",
    "    print('\\t\\tBefore Feature Selection\\tAfter Feature Selection')\n",
    "    print('No. of features:\\t', X_trainB.shape[1],'\\t\\t\\t',X_trainA.shape[1])\n",
    "    #print(\"Dataset Size(in MB):\\t\",(X_trainB.values.nbytes/1e6),'\\t\\t',(X_trainA.values.nbytes/1e6))\n",
    "\n",
    "    # training accuracy\n",
    "    train_predB = clf.predict(X_trainB)\n",
    "    train_predA = dlf.predict(X_trainA)\n",
    "    train_accB = round(accuracy_score(y_trainB,train_predB)*100, 2)\n",
    "    train_accA = round(accuracy_score(y_trainA,train_predA)*100, 2)\n",
    "    print('Train Accuracy:\\t\\t',train_accB,'\\t\\t\\t',train_accA)\n",
    "\n",
    "    # test accuracy \n",
    "    test_predB = clf.predict(X_testB)\n",
    "    test_predA = dlf.predict(X_testA)\n",
    "    test_accB = round(accuracy_score(y_testB,test_predB)*100, 2)\n",
    "    test_accA = round(accuracy_score(y_testA,test_predA)*100, 2)\n",
    "    print('Test Accuracy:\\t\\t',test_accB,'\\t\\t\\t',test_accA)\n",
    "        \n",
    "    # roc_auc_score\n",
    "    test_roc_aucB = round(roc_auc_score(y_testB,test_predB), 2)\n",
    "    test_roc_aucA = round(roc_auc_score(y_testA,test_predA), 2)\n",
    "    print('ROC AUC score:\\t\\t',test_roc_aucB,'\\t\\t\\t',test_roc_aucA)\n",
    "\n",
    "    # f1 score\n",
    "    test_f1B = round(f1_score(y_testB,test_predB),2)\n",
    "    test_f1A = round(f1_score(y_testA,test_predA),2)\n",
    "    print('f1_score:\\t\\t',test_f1B,'\\t\\t\\t',test_f1A)\n",
    "\n",
    "    # precision\n",
    "    test_precB = round(precision_score(y_testB,test_predB),2)\n",
    "    test_precA = round(precision_score(y_testA,test_predA),2)\n",
    "    print('Precision:\\t\\t',test_precB,'\\t\\t\\t',test_precA)\n",
    "\n",
    "    # recall\n",
    "    test_recallB = round(recall_score(y_testB,test_predB),2)\n",
    "    test_recallA = round(recall_score(y_testA,test_predA),2)\n",
    "    print('Recall:\\t\\t\\t',test_recallB,'\\t\\t\\t',test_recallA)\n",
    "\n",
    "    # Matthews correlation coefficient\n",
    "    test_MCCB = round(matthews_corrcoef(y_testB,test_predB),2)\n",
    "    test_MCCA = round(matthews_corrcoef(y_testA,test_predA),2)\n",
    "    print('MCC:\\t\\t\\t',test_MCCB,'\\t\\t\\t',test_MCCA)\n",
    "           \n",
    "    # training time\n",
    "    timeB = round((float(endB)- float(startB)),2)\n",
    "    timeA = round((float(endA)- float(startA)),2)\n",
    "    print('Train Time (in seconds):',timeB,'\\t\\t\\t',timeA)\n",
    "       \n",
    "    # confusion matrix\n",
    "    cm_resultB = confusion_matrix(y_testB,test_predB)\n",
    "    cm_resultA = confusion_matrix(y_testA,test_predA)\n",
    "    print('Confusion Matrix(Before):\\n',cm_resultB)\n",
    "    print('Confusion Matrix(after):\\n',cm_resultA)\n",
    "           \n",
    "    print('---------------------------------------------------------------------------------------------')\n",
    "    print('---------------------------------------------------------------------------------------------')\n",
    "    print('\\n')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. CL 404"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fature Ranking information\n",
      "---------------------------------------------------------------------------------------------\n",
      "            feature  importance\n",
      "0         C_f_Q1_Ay    0.046791\n",
      "1   C_t_variance_Ay    0.032198\n",
      "2       C_t_mean_Gx    0.031953\n",
      "3       C_t_mean_Az    0.028110\n",
      "4       L_t_mean_Gz    0.023688\n",
      "5   C_t_skewness_Az    0.023446\n",
      "6   L_t_skewness_Az    0.023240\n",
      "7        L_t_std_Gx    0.021365\n",
      "8        C_f_IQR_Az    0.021344\n",
      "9   L_t_kurtosis_Gy    0.020369\n",
      "10  C_t_variance_Ax    0.016207\n",
      "11      L_t_mean_Az    0.014290\n",
      "12       C_t_std_Gx    0.014111\n",
      "13  L_t_skewness_Gx    0.013753\n",
      "14  C_t_kurtosis_Gz    0.012685\n",
      "15  L_f_variance_Gx    0.012466\n",
      "16      L_t_mean_Ax    0.012236\n",
      "17        L_f_Q1_Ay    0.012192\n",
      "18      L_t_mean_Gy    0.011834\n",
      "19      C_t_mean_Gz    0.011466\n",
      "20  L_t_kurtosis_Az    0.011202\n",
      "21  C_t_kurtosis_Az    0.010681\n",
      "22  L_f_variance_Gz    0.010503\n",
      "23      C_f_mean_Gz    0.009738\n",
      "24        L_f_Q1_Ax    0.009472\n",
      "25  C_t_kurtosis_Gy    0.008928\n",
      "26      C_f_mean_Ay    0.008329\n",
      "27  C_f_variance_Gy    0.007380\n",
      "28       L_t_std_Gy    0.007345\n",
      "29  C_f_variance_Gx    0.007058\n",
      "30  L_t_kurtosis_Ax    0.007046\n",
      "31  C_t_kurtosis_Ay    0.007023\n",
      "32  C_t_kurtosis_Ax    0.006477\n",
      "33  C_t_skewness_Ax    0.005195\n",
      "34  C_t_skewness_Gy    0.004895\n",
      "35  L_t_skewness_Gz    0.004620\n",
      "36  L_t_kurtosis_Gz    0.004578\n",
      "37      C_t_mean_Gy    0.004241\n",
      "38  L_t_skewness_Gy    0.002297\n",
      "39  L_t_kurtosis_Ay    0.002182\n",
      "40  L_t_kurtosis_Gx    0.001897\n",
      "41  L_f_variance_Gy    0.001814\n",
      "42  C_f_kurtosis_Ax    0.001745\n",
      "43  C_t_skewness_Gz    0.001548\n",
      "44      L_t_mean_Gx    0.001181\n",
      "45  C_t_kurtosis_Gx    0.000859\n",
      "['C_f_Q1_Ay', 'C_t_variance_Ay', 'C_t_mean_Gx', 'C_t_mean_Az', 'L_t_mean_Gz', 'C_t_skewness_Az', 'L_t_skewness_Az', 'L_t_std_Gx', 'C_f_IQR_Az', 'L_t_kurtosis_Gy', 'C_t_variance_Ax', 'L_t_mean_Az', 'C_t_std_Gx', 'L_t_skewness_Gx', 'C_t_kurtosis_Gz', 'L_f_variance_Gx', 'L_t_mean_Ax', 'L_f_Q1_Ay', 'L_t_mean_Gy', 'C_t_mean_Gz', 'L_t_kurtosis_Az', 'C_t_kurtosis_Az', 'L_f_variance_Gz', 'C_f_mean_Gz', 'L_f_Q1_Ax', 'C_t_kurtosis_Gy', 'C_f_mean_Ay', 'C_f_variance_Gy', 'L_t_std_Gy', 'C_f_variance_Gx', 'L_t_kurtosis_Ax', 'C_t_kurtosis_Ay', 'C_t_kurtosis_Ax', 'C_t_skewness_Ax', 'C_t_skewness_Gy', 'L_t_skewness_Gz', 'L_t_kurtosis_Gz', 'C_t_mean_Gy', 'L_t_skewness_Gy', 'L_t_kurtosis_Ay', 'L_t_kurtosis_Gx', 'L_f_variance_Gy', 'C_f_kurtosis_Ax', 'C_t_skewness_Gz', 'L_t_mean_Gx', 'C_t_kurtosis_Gx']\n",
      "---------------------------------------------------------------------------------------------\n",
      "\t\t\t\tClassifier: DECISION TREE\n",
      "---------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------\n",
      "\t\tBefore Feature Selection\tAfter Feature Selection\n",
      "No. of features:\t 404 \t\t\t 15\n",
      "Train Accuracy:\t\t 83.01 \t\t\t 75.99\n",
      "Test Accuracy:\t\t 45.87 \t\t\t 75.71\n",
      "ROC AUC score:\t\t 0.35 \t\t\t 0.5\n",
      "f1_score:\t\t 0.61 \t\t\t 0.86\n",
      "Precision:\t\t 0.67 \t\t\t 0.76\n",
      "Recall:\t\t\t 0.56 \t\t\t 1.0\n",
      "MCC:\t\t\t -0.27 \t\t\t 0.0\n",
      "Train Time (in seconds): 19.25 \t\t\t 0.22\n",
      "Confusion Matrix(Before):\n",
      " [[ 188 1280]\n",
      " [1991 2584]]\n",
      "Confusion Matrix(after):\n",
      " [[   0 1468]\n",
      " [   0 4575]]\n",
      "---------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Load dataset as pandas data frame\n",
    "filename = 'CL_both_404.csv'\n",
    "dataset = pd.read_csv(filename)\n",
    "\n",
    "#Split data into input and output variable\n",
    "X = dataset.iloc[:,0:dataset.shape[1]-1]\n",
    "Y = dataset.iloc[:,-1]\n",
    "\n",
    "X_trainB, X_testB, y_trainB, y_testB = train_test_split(X, Y,test_size=0.3,random_state = 100)\n",
    "X_trainA, X_testA, y_trainA, y_testA = train_test_split(X, Y,test_size=0.3,random_state = 100)\n",
    "\n",
    "# Removing Constant features\n",
    "constant_filter = VarianceThreshold()\n",
    "constant_filter.fit(X_trainA)\n",
    "constant_columns = [col for col in X_trainA.columns\n",
    "                    if col not in X_trainA.columns[constant_filter.get_support()]]\n",
    "X_trainA.drop(labels=constant_columns,axis=1, inplace=True) \n",
    "X_testA.drop(labels=constant_columns,axis=1, inplace=True)\n",
    "\n",
    "# Removing Quasi-Constant features\n",
    "qconstant_filter = VarianceThreshold(0.01)\n",
    "qconstant_filter.fit(X_trainA)\n",
    "qconstant_columns = [col for col in X_trainA.columns\n",
    "                    if col not in X_trainA.columns[qconstant_filter.get_support()]]\n",
    "X_trainA.drop(labels=qconstant_columns,axis=1, inplace=True) \n",
    "X_testA.drop(labels=qconstant_columns,axis=1, inplace=True)\n",
    "\n",
    "# Removing Correlated Features\n",
    "correlated_features = set()  \n",
    "correlation_matrix = X_trainA.corr(method = 'pearson')\n",
    "for i in range(len(correlation_matrix.columns)):  \n",
    "    for j in range(i):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > 0.4:\n",
    "            colname = correlation_matrix.columns[i]\n",
    "            correlated_features.add(colname)\n",
    "X_trainA.drop(labels=correlated_features,axis=1, inplace=True) \n",
    "X_testA.drop(labels=correlated_features,axis=1, inplace=True)\n",
    "\n",
    "# feature ranking and selection    \n",
    "ranking_info, selected_features = feature_ranking_selection(X_trainA, y_trainA, 15)\n",
    "print('Fature Ranking information')\n",
    "print('---------------------------------------------------------------------------------------------')\n",
    "print(ranking_info)\n",
    "print(list(ranking_info['feature']))\n",
    "print('---------------------------------------------------------------------------------------------')\n",
    "X_trainA = X_trainA[selected_features]        \n",
    "X_testA = X_testA[selected_features] \n",
    "\n",
    "#names = [\"Nearest Neighbors\",\"Decision Tree\",\"Naive Bayes\"]\n",
    "names = [\"Decision Tree\"]\n",
    "\n",
    "classifiers = [\n",
    "    #KNeighborsClassifier(5, n_jobs= -1 ),\n",
    "    DecisionTreeClassifier(random_state = 100)\n",
    "    #GaussianNB(),\n",
    "   ]\n",
    "classifier2= [\n",
    "    #KNeighborsClassifier(5, n_jobs= -1 ),\n",
    "    DecisionTreeClassifier(max_depth = 5, min_samples_leaf = 50,random_state = 100)\n",
    "    #GaussianNB(),\n",
    "   ]\n",
    "clf_bef = list()\n",
    "clf_aft = list()\n",
    "\n",
    "for name, clf, dlf in zip(names,classifiers,classifier2):\n",
    "    # clf for before\n",
    "    # dlf for after\n",
    "    \n",
    "    # Before Feature Selection\n",
    "    startB = time.time()\n",
    "    clf.fit(X_trainB,y_trainB)    \n",
    "    endB = time.time() \n",
    "    clf_bef.append(clf)  \n",
    "    \n",
    "    # after Feature Selection    \n",
    "    startA = time.time()\n",
    "    dlf.fit(X_trainA,y_trainA)\n",
    "    endA = time.time()\n",
    "    clf_aft.append(dlf)\n",
    "        \n",
    "    print('\\t\\t\\t\\tClassifier:',name.upper())\n",
    "    print('---------------------------------------------------------------------------------------------')\n",
    "    print('---------------------------------------------------------------------------------------------')\n",
    "         \n",
    "    print('\\t\\tBefore Feature Selection\\tAfter Feature Selection')\n",
    "    print('No. of features:\\t', X_trainB.shape[1],'\\t\\t\\t',X_trainA.shape[1])\n",
    "    #print(\"Dataset Size(in MB):\\t\",(X_trainB.values.nbytes/1e6),'\\t\\t',(X_trainA.values.nbytes/1e6))\n",
    "\n",
    "    # training accuracy\n",
    "    train_predB = clf.predict(X_trainB)\n",
    "    train_predA = dlf.predict(X_trainA)\n",
    "    train_accB = round(accuracy_score(y_trainB,train_predB)*100, 2)\n",
    "    train_accA = round(accuracy_score(y_trainA,train_predA)*100, 2)\n",
    "    print('Train Accuracy:\\t\\t',train_accB,'\\t\\t\\t',train_accA)\n",
    "\n",
    "    # test accuracy \n",
    "    test_predB = clf.predict(X_testB)\n",
    "    test_predA = dlf.predict(X_testA)\n",
    "    test_accB = round(accuracy_score(y_testB,test_predB)*100, 2)\n",
    "    test_accA = round(accuracy_score(y_testA,test_predA)*100, 2)\n",
    "    print('Test Accuracy:\\t\\t',test_accB,'\\t\\t\\t',test_accA)\n",
    "        \n",
    "    # roc_auc_score\n",
    "    test_roc_aucB = round(roc_auc_score(y_testB,test_predB), 2)\n",
    "    test_roc_aucA = round(roc_auc_score(y_testA,test_predA), 2)\n",
    "    print('ROC AUC score:\\t\\t',test_roc_aucB,'\\t\\t\\t',test_roc_aucA)\n",
    "\n",
    "    # f1 score\n",
    "    test_f1B = round(f1_score(y_testB,test_predB),2)\n",
    "    test_f1A = round(f1_score(y_testA,test_predA),2)\n",
    "    print('f1_score:\\t\\t',test_f1B,'\\t\\t\\t',test_f1A)\n",
    "\n",
    "    # precision\n",
    "    test_precB = round(precision_score(y_testB,test_predB),2)\n",
    "    test_precA = round(precision_score(y_testA,test_predA),2)\n",
    "    print('Precision:\\t\\t',test_precB,'\\t\\t\\t',test_precA)\n",
    "\n",
    "    # recall\n",
    "    test_recallB = round(recall_score(y_testB,test_predB),2)\n",
    "    test_recallA = round(recall_score(y_testA,test_predA),2)\n",
    "    print('Recall:\\t\\t\\t',test_recallB,'\\t\\t\\t',test_recallA)\n",
    "\n",
    "    # Matthews correlation coefficient\n",
    "    test_MCCB = round(matthews_corrcoef(y_testB,test_predB),2)\n",
    "    test_MCCA = round(matthews_corrcoef(y_testA,test_predA),2)\n",
    "    print('MCC:\\t\\t\\t',test_MCCB,'\\t\\t\\t',test_MCCA)\n",
    "           \n",
    "    # training time\n",
    "    timeB = round((float(endB)- float(startB)),2)\n",
    "    timeA = round((float(endA)- float(startA)),2)\n",
    "    print('Train Time (in seconds):',timeB,'\\t\\t\\t',timeA)\n",
    "       \n",
    "    # confusion matrix\n",
    "    cm_resultB = confusion_matrix(y_testB,test_predB)\n",
    "    cm_resultA = confusion_matrix(y_testA,test_predA)\n",
    "    print('Confusion Matrix(Before):\\n',cm_resultB)\n",
    "    print('Confusion Matrix(after):\\n',cm_resultA)\n",
    "           \n",
    "    print('---------------------------------------------------------------------------------------------')\n",
    "    print('---------------------------------------------------------------------------------------------')\n",
    "    print('\\n')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. CR 404"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fature Ranking information\n",
      "---------------------------------------------------------------------------------------------\n",
      "            feature  importance\n",
      "0       R_t_mean_Ax    0.069289\n",
      "1         C_f_Q1_Ay    0.042156\n",
      "2       C_t_mean_Gx    0.035609\n",
      "3   C_t_variance_Ax    0.030929\n",
      "4   C_t_skewness_Az    0.030857\n",
      "5       R_t_mean_Ay    0.026200\n",
      "6       C_t_mean_Az    0.024570\n",
      "7       R_t_mean_Gz    0.023939\n",
      "8   R_t_skewness_Az    0.021900\n",
      "9   C_t_kurtosis_Gz    0.020255\n",
      "10       R_t_std_Gy    0.018617\n",
      "11  C_t_variance_Ay    0.016914\n",
      "12  R_t_kurtosis_Az    0.015552\n",
      "13  R_t_skewness_Gy    0.015426\n",
      "14        R_f_Q1_Ax    0.014831\n",
      "15  R_f_variance_Gz    0.014493\n",
      "16       C_f_IQR_Az    0.013719\n",
      "17  C_t_kurtosis_Az    0.013691\n",
      "18       C_t_std_Gx    0.013046\n",
      "19        R_f_Q1_Ay    0.012886\n",
      "20      C_f_mean_Ay    0.010258\n",
      "21  C_t_kurtosis_Gy    0.009874\n",
      "22  C_f_variance_Gy    0.009365\n",
      "23  C_t_kurtosis_Ay    0.009164\n",
      "24  R_t_kurtosis_Ay    0.009038\n",
      "25      C_t_mean_Gz    0.008752\n",
      "26  R_t_skewness_Gz    0.008699\n",
      "27  R_t_kurtosis_Gy    0.008467\n",
      "28      C_f_mean_Gz    0.008326\n",
      "29      R_t_mean_Gy    0.008300\n",
      "30  C_t_skewness_Gz    0.008067\n",
      "31      R_t_mean_Az    0.007506\n",
      "32  C_f_variance_Gx    0.006921\n",
      "33  C_t_kurtosis_Gx    0.006886\n",
      "34  C_f_kurtosis_Ax    0.006740\n",
      "35  R_t_kurtosis_Gx    0.004992\n",
      "36  R_t_skewness_Ax    0.004703\n",
      "37  C_t_kurtosis_Ax    0.004661\n",
      "38  R_t_kurtosis_Ax    0.004176\n",
      "39       R_t_std_Gx    0.003023\n",
      "40  C_t_skewness_Gy    0.002865\n",
      "41  R_f_variance_Gx    0.002297\n",
      "42  R_f_variance_Gy    0.001635\n",
      "43      R_t_mean_Gx    0.000927\n",
      "44      C_t_mean_Gy    0.000733\n",
      "45  R_t_kurtosis_Gz    0.000248\n",
      "['R_t_mean_Ax', 'C_f_Q1_Ay', 'C_t_mean_Gx', 'C_t_variance_Ax', 'C_t_skewness_Az', 'R_t_mean_Ay', 'C_t_mean_Az', 'R_t_mean_Gz', 'R_t_skewness_Az', 'C_t_kurtosis_Gz', 'R_t_std_Gy', 'C_t_variance_Ay', 'R_t_kurtosis_Az', 'R_t_skewness_Gy', 'R_f_Q1_Ax', 'R_f_variance_Gz', 'C_f_IQR_Az', 'C_t_kurtosis_Az', 'C_t_std_Gx', 'R_f_Q1_Ay', 'C_f_mean_Ay', 'C_t_kurtosis_Gy', 'C_f_variance_Gy', 'C_t_kurtosis_Ay', 'R_t_kurtosis_Ay', 'C_t_mean_Gz', 'R_t_skewness_Gz', 'R_t_kurtosis_Gy', 'C_f_mean_Gz', 'R_t_mean_Gy', 'C_t_skewness_Gz', 'R_t_mean_Az', 'C_f_variance_Gx', 'C_t_kurtosis_Gx', 'C_f_kurtosis_Ax', 'R_t_kurtosis_Gx', 'R_t_skewness_Ax', 'C_t_kurtosis_Ax', 'R_t_kurtosis_Ax', 'R_t_std_Gx', 'C_t_skewness_Gy', 'R_f_variance_Gx', 'R_f_variance_Gy', 'R_t_mean_Gx', 'C_t_mean_Gy', 'R_t_kurtosis_Gz']\n",
      "---------------------------------------------------------------------------------------------\n",
      "\t\t\t\tClassifier: DECISION TREE\n",
      "---------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------\n",
      "\t\tBefore Feature Selection\tAfter Feature Selection\n",
      "No. of features:\t 404 \t\t\t 15\n",
      "Train Accuracy:\t\t 83.05 \t\t\t 75.81\n",
      "Test Accuracy:\t\t 45.54 \t\t\t 76.12\n",
      "ROC AUC score:\t\t 0.33 \t\t\t 0.5\n",
      "f1_score:\t\t 0.61 \t\t\t 0.86\n",
      "Precision:\t\t 0.67 \t\t\t 0.76\n",
      "Recall:\t\t\t 0.57 \t\t\t 1.0\n",
      "MCC:\t\t\t -0.3 \t\t\t 0.0\n",
      "Train Time (in seconds): 17.97 \t\t\t 0.2\n",
      "Confusion Matrix(Before):\n",
      " [[ 143 1300]\n",
      " [1991 2609]]\n",
      "Confusion Matrix(after):\n",
      " [[   0 1443]\n",
      " [   0 4600]]\n",
      "---------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Load dataset as pandas data frame\n",
    "filename = 'CR_both_404.csv'\n",
    "dataset = pd.read_csv(filename)\n",
    "\n",
    "#Split data into input and output variable\n",
    "X = dataset.iloc[:,0:dataset.shape[1]-1]\n",
    "Y = dataset.iloc[:,-1]\n",
    "\n",
    "X_trainB, X_testB, y_trainB, y_testB = train_test_split(X, Y,test_size=0.3,random_state = 100)\n",
    "X_trainA, X_testA, y_trainA, y_testA = train_test_split(X, Y,test_size=0.3,random_state = 100)\n",
    "\n",
    "# Removing Constant features\n",
    "constant_filter = VarianceThreshold()\n",
    "constant_filter.fit(X_trainA)\n",
    "constant_columns = [col for col in X_trainA.columns\n",
    "                    if col not in X_trainA.columns[constant_filter.get_support()]]\n",
    "X_trainA.drop(labels=constant_columns,axis=1, inplace=True) \n",
    "X_testA.drop(labels=constant_columns,axis=1, inplace=True)\n",
    "\n",
    "# Removing Quasi-Constant features\n",
    "qconstant_filter = VarianceThreshold(0.01)\n",
    "qconstant_filter.fit(X_trainA)\n",
    "qconstant_columns = [col for col in X_trainA.columns\n",
    "                    if col not in X_trainA.columns[qconstant_filter.get_support()]]\n",
    "X_trainA.drop(labels=qconstant_columns,axis=1, inplace=True) \n",
    "X_testA.drop(labels=qconstant_columns,axis=1, inplace=True)\n",
    "\n",
    "# Removing Correlated Features\n",
    "correlated_features = set()  \n",
    "correlation_matrix = X_trainA.corr(method = 'pearson')\n",
    "for i in range(len(correlation_matrix.columns)):  \n",
    "    for j in range(i):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > 0.4:\n",
    "            colname = correlation_matrix.columns[i]\n",
    "            correlated_features.add(colname)\n",
    "X_trainA.drop(labels=correlated_features,axis=1, inplace=True) \n",
    "X_testA.drop(labels=correlated_features,axis=1, inplace=True)\n",
    "\n",
    "# feature ranking and selection    \n",
    "ranking_info, selected_features = feature_ranking_selection(X_trainA, y_trainA, 15)\n",
    "print('Fature Ranking information')\n",
    "print('---------------------------------------------------------------------------------------------')\n",
    "print(ranking_info)\n",
    "print(list(ranking_info['feature']))\n",
    "print('---------------------------------------------------------------------------------------------')\n",
    "X_trainA = X_trainA[selected_features]        \n",
    "X_testA = X_testA[selected_features] \n",
    "\n",
    "#names = [\"Nearest Neighbors\",\"Decision Tree\",\"Naive Bayes\"]\n",
    "names = [\"Decision Tree\"]\n",
    "\n",
    "classifiers = [\n",
    "    #KNeighborsClassifier(5, n_jobs= -1 ),\n",
    "    DecisionTreeClassifier(random_state = 100)\n",
    "    #GaussianNB(),\n",
    "   ]\n",
    "classifier2= [\n",
    "    #KNeighborsClassifier(5, n_jobs= -1 ),\n",
    "    DecisionTreeClassifier(max_depth = 5, min_samples_leaf = 50,random_state = 100)\n",
    "    #GaussianNB(),\n",
    "   ]\n",
    "clf_bef = list()\n",
    "clf_aft = list()\n",
    "\n",
    "for name, clf, dlf in zip(names,classifiers,classifier2):\n",
    "    # clf for before\n",
    "    # dlf for after\n",
    "    \n",
    "    # Before Feature Selection\n",
    "    startB = time.time()\n",
    "    clf.fit(X_trainB,y_trainB)    \n",
    "    endB = time.time() \n",
    "    clf_bef.append(clf)  \n",
    "    \n",
    "    # after Feature Selection    \n",
    "    startA = time.time()\n",
    "    dlf.fit(X_trainA,y_trainA)\n",
    "    endA = time.time()\n",
    "    clf_aft.append(dlf)\n",
    "        \n",
    "    print('\\t\\t\\t\\tClassifier:',name.upper())\n",
    "    print('---------------------------------------------------------------------------------------------')\n",
    "    print('---------------------------------------------------------------------------------------------')\n",
    "         \n",
    "    print('\\t\\tBefore Feature Selection\\tAfter Feature Selection')\n",
    "    print('No. of features:\\t', X_trainB.shape[1],'\\t\\t\\t',X_trainA.shape[1])\n",
    "    #print(\"Dataset Size(in MB):\\t\",(X_trainB.values.nbytes/1e6),'\\t\\t',(X_trainA.values.nbytes/1e6))\n",
    "\n",
    "    # training accuracy\n",
    "    train_predB = clf.predict(X_trainB)\n",
    "    train_predA = dlf.predict(X_trainA)\n",
    "    train_accB = round(accuracy_score(y_trainB,train_predB)*100, 2)\n",
    "    train_accA = round(accuracy_score(y_trainA,train_predA)*100, 2)\n",
    "    print('Train Accuracy:\\t\\t',train_accB,'\\t\\t\\t',train_accA)\n",
    "\n",
    "    # test accuracy \n",
    "    test_predB = clf.predict(X_testB)\n",
    "    test_predA = dlf.predict(X_testA)\n",
    "    test_accB = round(accuracy_score(y_testB,test_predB)*100, 2)\n",
    "    test_accA = round(accuracy_score(y_testA,test_predA)*100, 2)\n",
    "    print('Test Accuracy:\\t\\t',test_accB,'\\t\\t\\t',test_accA)\n",
    "        \n",
    "    # roc_auc_score\n",
    "    test_roc_aucB = round(roc_auc_score(y_testB,test_predB), 2)\n",
    "    test_roc_aucA = round(roc_auc_score(y_testA,test_predA), 2)\n",
    "    print('ROC AUC score:\\t\\t',test_roc_aucB,'\\t\\t\\t',test_roc_aucA)\n",
    "\n",
    "    # f1 score\n",
    "    test_f1B = round(f1_score(y_testB,test_predB),2)\n",
    "    test_f1A = round(f1_score(y_testA,test_predA),2)\n",
    "    print('f1_score:\\t\\t',test_f1B,'\\t\\t\\t',test_f1A)\n",
    "\n",
    "    # precision\n",
    "    test_precB = round(precision_score(y_testB,test_predB),2)\n",
    "    test_precA = round(precision_score(y_testA,test_predA),2)\n",
    "    print('Precision:\\t\\t',test_precB,'\\t\\t\\t',test_precA)\n",
    "\n",
    "    # recall\n",
    "    test_recallB = round(recall_score(y_testB,test_predB),2)\n",
    "    test_recallA = round(recall_score(y_testA,test_predA),2)\n",
    "    print('Recall:\\t\\t\\t',test_recallB,'\\t\\t\\t',test_recallA)\n",
    "\n",
    "    # Matthews correlation coefficient\n",
    "    test_MCCB = round(matthews_corrcoef(y_testB,test_predB),2)\n",
    "    test_MCCA = round(matthews_corrcoef(y_testA,test_predA),2)\n",
    "    print('MCC:\\t\\t\\t',test_MCCB,'\\t\\t\\t',test_MCCA)\n",
    "           \n",
    "    # training time\n",
    "    timeB = round((float(endB)- float(startB)),2)\n",
    "    timeA = round((float(endA)- float(startA)),2)\n",
    "    print('Train Time (in seconds):',timeB,'\\t\\t\\t',timeA)\n",
    "       \n",
    "    # confusion matrix\n",
    "    cm_resultB = confusion_matrix(y_testB,test_predB)\n",
    "    cm_resultA = confusion_matrix(y_testA,test_predA)\n",
    "    print('Confusion Matrix(Before):\\n',cm_resultB)\n",
    "    print('Confusion Matrix(after):\\n',cm_resultA)\n",
    "           \n",
    "    print('---------------------------------------------------------------------------------------------')\n",
    "    print('---------------------------------------------------------------------------------------------')\n",
    "    print('\\n')   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. LR 404"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fature Ranking information\n",
      "---------------------------------------------------------------------------------------------\n",
      "            feature  importance\n",
      "0       R_t_mean_Ax    0.067492\n",
      "1       R_t_mean_Ay    0.028052\n",
      "2   R_t_skewness_Az    0.026928\n",
      "3        L_t_std_Gx    0.026367\n",
      "4       L_t_mean_Gz    0.025095\n",
      "5   R_t_kurtosis_Gy    0.021130\n",
      "6       R_t_mean_Gz    0.020045\n",
      "7   R_t_kurtosis_Gx    0.017984\n",
      "8         L_f_Q1_Ay    0.017468\n",
      "9   R_t_kurtosis_Az    0.015303\n",
      "10      L_t_mean_Ax    0.015276\n",
      "11  R_t_skewness_Gz    0.015097\n",
      "12      R_t_mean_Az    0.014886\n",
      "13  L_t_skewness_Az    0.014066\n",
      "14  L_t_kurtosis_Gy    0.014054\n",
      "15  R_f_variance_Gz    0.013357\n",
      "16  L_t_kurtosis_Gz    0.013048\n",
      "17      L_t_mean_Az    0.012387\n",
      "18  L_t_skewness_Gx    0.012260\n",
      "19        R_f_Q1_Ax    0.011180\n",
      "20      R_t_mean_Gy    0.009919\n",
      "21  R_t_skewness_Gy    0.009317\n",
      "22       R_t_std_Gx    0.008905\n",
      "23  L_f_variance_Gz    0.008889\n",
      "24       R_t_std_Gy    0.008728\n",
      "25  R_t_skewness_Ax    0.008038\n",
      "26       L_t_std_Gy    0.007981\n",
      "27        R_f_Q1_Ay    0.007955\n",
      "28        L_f_Q1_Ax    0.007922\n",
      "29      R_t_mean_Gx    0.006821\n",
      "30  L_t_kurtosis_Ax    0.006172\n",
      "31  L_t_kurtosis_Az    0.005955\n",
      "32  L_t_skewness_Gy    0.005914\n",
      "33  L_f_variance_Gx    0.005470\n",
      "34  L_t_skewness_Gz    0.005397\n",
      "35  L_f_variance_Gy    0.004870\n",
      "36  R_t_kurtosis_Ax    0.004770\n",
      "37  L_t_kurtosis_Ay    0.003403\n",
      "38  R_t_kurtosis_Ay    0.003349\n",
      "39      L_t_mean_Gx    0.002510\n",
      "40  R_t_kurtosis_Gz    0.001697\n",
      "41      L_t_mean_Gy    0.001399\n",
      "42  R_f_variance_Gx    0.001202\n",
      "43  R_f_variance_Gy    0.000469\n",
      "44  L_t_kurtosis_Gx    0.000189\n",
      "['R_t_mean_Ax', 'R_t_mean_Ay', 'R_t_skewness_Az', 'L_t_std_Gx', 'L_t_mean_Gz', 'R_t_kurtosis_Gy', 'R_t_mean_Gz', 'R_t_kurtosis_Gx', 'L_f_Q1_Ay', 'R_t_kurtosis_Az', 'L_t_mean_Ax', 'R_t_skewness_Gz', 'R_t_mean_Az', 'L_t_skewness_Az', 'L_t_kurtosis_Gy', 'R_f_variance_Gz', 'L_t_kurtosis_Gz', 'L_t_mean_Az', 'L_t_skewness_Gx', 'R_f_Q1_Ax', 'R_t_mean_Gy', 'R_t_skewness_Gy', 'R_t_std_Gx', 'L_f_variance_Gz', 'R_t_std_Gy', 'R_t_skewness_Ax', 'L_t_std_Gy', 'R_f_Q1_Ay', 'L_f_Q1_Ax', 'R_t_mean_Gx', 'L_t_kurtosis_Ax', 'L_t_kurtosis_Az', 'L_t_skewness_Gy', 'L_f_variance_Gx', 'L_t_skewness_Gz', 'L_f_variance_Gy', 'R_t_kurtosis_Ax', 'L_t_kurtosis_Ay', 'R_t_kurtosis_Ay', 'L_t_mean_Gx', 'R_t_kurtosis_Gz', 'L_t_mean_Gy', 'R_f_variance_Gx', 'R_f_variance_Gy', 'L_t_kurtosis_Gx']\n",
      "---------------------------------------------------------------------------------------------\n",
      "\t\t\t\tClassifier: DECISION TREE\n",
      "---------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------\n",
      "\t\tBefore Feature Selection\tAfter Feature Selection\n",
      "No. of features:\t 404 \t\t\t 15\n",
      "Train Accuracy:\t\t 83.24 \t\t\t 75.68\n",
      "Test Accuracy:\t\t 45.04 \t\t\t 76.1\n",
      "ROC AUC score:\t\t 0.33 \t\t\t 0.5\n",
      "f1_score:\t\t 0.61 \t\t\t 0.86\n",
      "Precision:\t\t 0.67 \t\t\t 0.76\n",
      "Recall:\t\t\t 0.55 \t\t\t 1.0\n",
      "MCC:\t\t\t -0.29 \t\t\t -0.02\n",
      "Train Time (in seconds): 17.01 \t\t\t 0.34\n",
      "Confusion Matrix(Before):\n",
      " [[ 165 1259]\n",
      " [2062 2557]]\n",
      "Confusion Matrix(after):\n",
      " [[   2 1422]\n",
      " [  22 4597]]\n",
      "---------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filename = 'LR_both_404.csv'\n",
    "dataset = pd.read_csv(filename)\n",
    "\n",
    "#Split data into input and output variable\n",
    "X = dataset.iloc[:,0:dataset.shape[1]-1]\n",
    "Y = dataset.iloc[:,-1]\n",
    "\n",
    "X_trainB, X_testB, y_trainB, y_testB = train_test_split(X, Y,test_size=0.3,random_state = 100)\n",
    "X_trainA, X_testA, y_trainA, y_testA = train_test_split(X, Y,test_size=0.3,random_state = 100)\n",
    "\n",
    "# Removing Constant features\n",
    "constant_filter = VarianceThreshold()\n",
    "constant_filter.fit(X_trainA)\n",
    "constant_columns = [col for col in X_trainA.columns\n",
    "                    if col not in X_trainA.columns[constant_filter.get_support()]]\n",
    "X_trainA.drop(labels=constant_columns,axis=1, inplace=True) \n",
    "X_testA.drop(labels=constant_columns,axis=1, inplace=True)\n",
    "\n",
    "# Removing Quasi-Constant features\n",
    "qconstant_filter = VarianceThreshold(0.01)\n",
    "qconstant_filter.fit(X_trainA)\n",
    "qconstant_columns = [col for col in X_trainA.columns\n",
    "                    if col not in X_trainA.columns[qconstant_filter.get_support()]]\n",
    "X_trainA.drop(labels=qconstant_columns,axis=1, inplace=True) \n",
    "X_testA.drop(labels=qconstant_columns,axis=1, inplace=True)\n",
    "\n",
    "# Removing Correlated Features\n",
    "correlated_features = set()  \n",
    "correlation_matrix = X_trainA.corr(method = 'pearson')\n",
    "for i in range(len(correlation_matrix.columns)):  \n",
    "    for j in range(i):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > 0.4:\n",
    "            colname = correlation_matrix.columns[i]\n",
    "            correlated_features.add(colname)\n",
    "X_trainA.drop(labels=correlated_features,axis=1, inplace=True) \n",
    "X_testA.drop(labels=correlated_features,axis=1, inplace=True)\n",
    "\n",
    "# feature ranking and selection    \n",
    "ranking_info, selected_features = feature_ranking_selection(X_trainA, y_trainA, 15)\n",
    "print('Fature Ranking information')\n",
    "print('---------------------------------------------------------------------------------------------')\n",
    "print(ranking_info)\n",
    "print(list(ranking_info['feature']))\n",
    "print('---------------------------------------------------------------------------------------------')\n",
    "X_trainA = X_trainA[selected_features]        \n",
    "X_testA = X_testA[selected_features] \n",
    "\n",
    "#names = [\"Nearest Neighbors\",\"Decision Tree\",\"Naive Bayes\"]\n",
    "names = [\"Decision Tree\"]\n",
    "\n",
    "classifiers = [\n",
    "    #KNeighborsClassifier(5, n_jobs= -1 ),\n",
    "    DecisionTreeClassifier(random_state = 100)\n",
    "    #GaussianNB(),\n",
    "   ]\n",
    "classifier2= [\n",
    "    #KNeighborsClassifier(5, n_jobs= -1 ),\n",
    "    DecisionTreeClassifier(max_depth = 5, min_samples_leaf = 50,random_state = 100)\n",
    "    #GaussianNB(),\n",
    "   ]\n",
    "clf_bef = list()\n",
    "clf_aft = list()\n",
    "\n",
    "for name, clf, dlf in zip(names,classifiers,classifier2):\n",
    "    # clf for before\n",
    "    # dlf for after\n",
    "    \n",
    "    # Before Feature Selection\n",
    "    startB = time.time()\n",
    "    clf.fit(X_trainB,y_trainB)    \n",
    "    endB = time.time() \n",
    "    clf_bef.append(clf)  \n",
    "    \n",
    "    # after Feature Selection    \n",
    "    startA = time.time()\n",
    "    dlf.fit(X_trainA,y_trainA)\n",
    "    endA = time.time()\n",
    "    clf_aft.append(dlf)\n",
    "        \n",
    "    print('\\t\\t\\t\\tClassifier:',name.upper())\n",
    "    print('---------------------------------------------------------------------------------------------')\n",
    "    print('---------------------------------------------------------------------------------------------')\n",
    "         \n",
    "    print('\\t\\tBefore Feature Selection\\tAfter Feature Selection')\n",
    "    print('No. of features:\\t', X_trainB.shape[1],'\\t\\t\\t',X_trainA.shape[1])\n",
    "    #print(\"Dataset Size(in MB):\\t\",(X_trainB.values.nbytes/1e6),'\\t\\t',(X_trainA.values.nbytes/1e6))\n",
    "\n",
    "    # training accuracy\n",
    "    train_predB = clf.predict(X_trainB)\n",
    "    train_predA = dlf.predict(X_trainA)\n",
    "    train_accB = round(accuracy_score(y_trainB,train_predB)*100, 2)\n",
    "    train_accA = round(accuracy_score(y_trainA,train_predA)*100, 2)\n",
    "    print('Train Accuracy:\\t\\t',train_accB,'\\t\\t\\t',train_accA)\n",
    "\n",
    "    # test accuracy \n",
    "    test_predB = clf.predict(X_testB)\n",
    "    test_predA = dlf.predict(X_testA)\n",
    "    test_accB = round(accuracy_score(y_testB,test_predB)*100, 2)\n",
    "    test_accA = round(accuracy_score(y_testA,test_predA)*100, 2)\n",
    "    print('Test Accuracy:\\t\\t',test_accB,'\\t\\t\\t',test_accA)\n",
    "        \n",
    "    # roc_auc_score\n",
    "    test_roc_aucB = round(roc_auc_score(y_testB,test_predB), 2)\n",
    "    test_roc_aucA = round(roc_auc_score(y_testA,test_predA), 2)\n",
    "    print('ROC AUC score:\\t\\t',test_roc_aucB,'\\t\\t\\t',test_roc_aucA)\n",
    "\n",
    "    # f1 score\n",
    "    test_f1B = round(f1_score(y_testB,test_predB),2)\n",
    "    test_f1A = round(f1_score(y_testA,test_predA),2)\n",
    "    print('f1_score:\\t\\t',test_f1B,'\\t\\t\\t',test_f1A)\n",
    "\n",
    "    # precision\n",
    "    test_precB = round(precision_score(y_testB,test_predB),2)\n",
    "    test_precA = round(precision_score(y_testA,test_predA),2)\n",
    "    print('Precision:\\t\\t',test_precB,'\\t\\t\\t',test_precA)\n",
    "\n",
    "    # recall\n",
    "    test_recallB = round(recall_score(y_testB,test_predB),2)\n",
    "    test_recallA = round(recall_score(y_testA,test_predA),2)\n",
    "    print('Recall:\\t\\t\\t',test_recallB,'\\t\\t\\t',test_recallA)\n",
    "\n",
    "    # Matthews correlation coefficient\n",
    "    test_MCCB = round(matthews_corrcoef(y_testB,test_predB),2)\n",
    "    test_MCCA = round(matthews_corrcoef(y_testA,test_predA),2)\n",
    "    print('MCC:\\t\\t\\t',test_MCCB,'\\t\\t\\t',test_MCCA)\n",
    "           \n",
    "    # training time\n",
    "    timeB = round((float(endB)- float(startB)),2)\n",
    "    timeA = round((float(endA)- float(startA)),2)\n",
    "    print('Train Time (in seconds):',timeB,'\\t\\t\\t',timeA)\n",
    "       \n",
    "    # confusion matrix\n",
    "    cm_resultB = confusion_matrix(y_testB,test_predB)\n",
    "    cm_resultA = confusion_matrix(y_testA,test_predA)\n",
    "    print('Confusion Matrix(Before):\\n',cm_resultB)\n",
    "    print('Confusion Matrix(after):\\n',cm_resultA)\n",
    "           \n",
    "    print('---------------------------------------------------------------------------------------------')\n",
    "    print('---------------------------------------------------------------------------------------------')\n",
    "    print('\\n')   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
